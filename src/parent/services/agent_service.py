# -*- coding: utf-8 -*-
"""
Parent Agent Service
OpenAI Function Calling ê¸°ë°˜ í•™ë¶€ëª¨ ìƒë‹´ ì‹œìŠ¤í…œ
"""
from __future__ import annotations
import os
import json
from typing import List, Dict, Any, Optional
from openai import OpenAI
from shared.services import get_graph_rag_service
from shared.prompts import PromptManager


class ParentAgentService:
    """í•™ë¶€ëª¨ ë§ì¶¤ Function Calling ì—ì´ì „íŠ¸"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=self.openai_api_key)
        self.graph_rag_service = get_graph_rag_service()

        # Current user message for vector search context
        self.current_user_message = ""

        # Function definitions
        self.functions = self._create_functions()

    def _create_functions(self) -> List[Dict[str, Any]]:
        """OpenAI Function Callingìš© function ì •ì˜"""
        return [
            {
                "type": "function",
                "function": {
                    "name": "get_child_info",
                    "description": "ìë…€ì˜ ìƒì„¸ í•™ìŠµ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì´ë¦„, í•™ë…„, CEFR ë ˆë²¨, ê°•ì , ì•½ì , ì˜ì—­ë³„ ì ìˆ˜, ì¶œì„ë¥ , ìˆ™ì œ ì™„ë£Œìœ¨, í•™ìŠµ íƒœë„, ë°˜ ì •ë³´ ë“±)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "ìë…€ì˜ í•™ìƒ ID (ì˜ˆ: S-01)"
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "analyze_performance",
                    "description": "ìë…€ì˜ ì„±ì  ë¶„ì„ ë° í•™ìŠµ ì¶”ì´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì˜ì—­ë³„ ê°•ì•½ì , ë˜ë˜ ë¹„êµ, ê°œì„  ì¶”ì´)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "ìë…€ì˜ í•™ìƒ ID"
                            },
                            "focus_area": {
                                "type": "string",
                                "description": "ì§‘ì¤‘ ë¶„ì„í•  ì˜ì—­ (ë…í•´, ë¬¸ë²•, ì–´íœ˜, ë“£ê¸°, ì“°ê¸° ì¤‘ í•˜ë‚˜, ë˜ëŠ” nullì¼ ê²½ìš° ì „ì²´ ë¶„ì„)",
                                "enum": ["ë…í•´", "ë¬¸ë²•", "ì–´íœ˜", "ë“£ê¸°", "ì“°ê¸°", None]
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_study_advice",
                    "description": "ìë…€ì˜ ì•½ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ ë§ì¶¤í˜• í•™ìŠµ ì¡°ì–¸ ë° ê°€ì • í•™ìŠµ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "ìë…€ì˜ í•™ìƒ ID"
                            },
                            "area": {
                                "type": "string",
                                "description": "ì¡°ì–¸ì´ í•„ìš”í•œ ì˜ì—­ (ë…í•´, ë¬¸ë²•, ì–´íœ˜, ë“£ê¸°, ì“°ê¸° ì¤‘ í•˜ë‚˜, ë˜ëŠ” nullì¼ ê²½ìš° ì „ì²´ ì•½ì  ê¸°ë°˜)",
                                "enum": ["ë…í•´", "ë¬¸ë²•", "ì–´íœ˜", "ë“£ê¸°", "ì“°ê¸°", None]
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_attendance_status",
                    "description": "ìë…€ì˜ ì¶œì„ í˜„í™©, ìˆ™ì œ ì™„ë£Œìœ¨, í•™ìŠµ íƒœë„ ë“± ì¼ì¼ í•™ìŠµ ê¸°ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "ìë…€ì˜ í•™ìƒ ID"
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "recommend_improvement_areas",
                    "description": "ìë…€ê°€ ì§‘ì¤‘ì ìœ¼ë¡œ ê°œì„ í•´ì•¼ í•  ì˜ì—­ê³¼ êµ¬ì²´ì ì¸ í•™ìŠµ ê³„íšì„ ì¶”ì²œí•©ë‹ˆë‹¤",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "ìë…€ì˜ í•™ìƒ ID"
                            },
                            "priority": {
                                "type": "string",
                                "description": "ìš°ì„ ìˆœìœ„ ê¸°ì¤€ (urgent: ì‹œê¸‰í•œ ì•½ì , balanced: ê· í˜• ì¡íŒ í•™ìŠµ, strength: ê°•ì  ê°•í™”)",
                                "enum": ["urgent", "balanced", "strength"]
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            }
        ]

    def _get_child_info(self, student_id: str, query_text: str = "í•™ìƒ ì •ë³´ ì¡°íšŒ") -> str:
        """ìë…€ ì •ë³´ ì¡°íšŒ ì‹¤í–‰ (ë²¡í„° ê²€ìƒ‰ í¬í•¨)"""
        try:
            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text=query_text,
                use_vector_search=True
            )

            # í•™ë¶€ëª¨ ê´€ì ì—ì„œ ì •ë³´ ì¬êµ¬ì„±
            if context:
                import re
                # ì´ë¦„ ì¶”ì¶œ
                name_match = re.search(r'ì´ë¦„: (.+)', context)
                student_name = name_match.group(1) if name_match else "ìë…€"

                # CEFR ë ˆë²¨ ì¶”ì¶œ
                cefr_match = re.search(r'CEFR ë ˆë²¨: ([A-C][1-2])', context)
                cefr_level = cefr_match.group(1) if cefr_match else "N/A"

                return f"""**{student_name} í•™ìƒì˜ í•™ìŠµ í˜„í™© (í•™ë¶€ëª¨ë‹˜ ê´€ì ):**

{context}

**í•™ë¶€ëª¨ë‹˜ê»˜ ì•ˆë‚´ ì‚¬í•­:**
- í˜„ì¬ ìë…€ì˜ CEFR ë ˆë²¨ì€ {cefr_level}ì…ë‹ˆë‹¤
- ì •ê¸°ì ì¸ ì¶œì„ê³¼ ìˆ™ì œ ì™„ë£Œê°€ ì„±ì  í–¥ìƒì— ì¤‘ìš”í•©ë‹ˆë‹¤
- ê°€ì •ì—ì„œë„ ê¾¸ì¤€í•œ í•™ìŠµ ìŠµê´€ í˜•ì„±ì´ í•„ìš”í•©ë‹ˆë‹¤
"""
            return context
        except Exception as e:
            return f"ìë…€ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    def _analyze_performance(self, student_id: str, focus_area: Optional[str] = None) -> str:
        """ì„±ì  ë¶„ì„ ì‹¤í–‰"""
        try:
            # GraphRAGë¥¼ í†µí•´ ì„±ì  ì •ë³´ ì¡°íšŒ
            query = f"ìë…€ì˜ {focus_area} ì„±ì  ë¶„ì„" if focus_area else "ìë…€ì˜ ì „ì²´ ì„±ì  ë¶„ì„ ë° ë˜ë˜ ë¹„êµ"
            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text=query,
                use_vector_search=True
            )

            # ì„±ì  ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
            analysis = f"""**ì„±ì  ë¶„ì„ ë¦¬í¬íŠ¸**

{context}

**ë˜ë˜ ë¹„êµ ë¶„ì„:**
- ê°™ì€ ë°˜ í•™ìƒë“¤ê³¼ ë¹„êµí•˜ì—¬ ìƒìœ„ê¶Œ/ì¤‘ìœ„ê¶Œ/í•˜ìœ„ê¶Œ íŒŒì•…
- ì˜ì—­ë³„ ê°•ì•½ì  ë¶„ì„

**ê°œì„  ì¶”ì´:**
- ìµœê·¼ í•™ìŠµ í™œë™ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ì„±ì¥ ê°€ëŠ¥ì„± ì˜ˆì¸¡
- ê¾¸ì¤€í•œ í•™ìŠµì´ í•„ìš”í•œ ì˜ì—­ ì‹ë³„
"""
            return analysis

        except Exception as e:
            return f"ì„±ì  ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    def _get_study_advice(self, student_id: str, area: Optional[str] = None) -> str:
        """í•™ìŠµ ì¡°ì–¸ ì œê³µ"""
        try:
            # GraphRAGë¥¼ í†µí•´ ì•½ì  ì˜ì—­ íŒŒì•…
            query = f"{area} ì˜ì—­ ê°œì„  ë°©ì•ˆ" if area else "ì „ì²´ ì•½ì  ê°œì„  ë°©ì•ˆ"
            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text=query,
                use_vector_search=True
            )

            # í•™ìŠµ ì¡°ì–¸ ìƒì„± (GPT-4o ì‚¬ìš©)
            advice_prompt = f"""ë‹¹ì‹ ì€ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í•™ìƒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ë¶€ëª¨ë‹˜ê»˜ ë§ì¶¤í˜• í•™ìŠµ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.

**í•™ìƒ ì •ë³´:**
{context}

**ì¡°ì–¸ ì˜ì—­:**
{area if area else "ì „ì²´ ì˜ì—­"}

**ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:**

1. **í˜„ì¬ ìƒí™© ìš”ì•½**
   - ìë…€ì˜ ê°•ì ê³¼ ì•½ì  ê°„ëµíˆ ì •ë¦¬

2. **ê°€ì •ì—ì„œ ë„ì™€ì£¼ì‹¤ ìˆ˜ ìˆëŠ” ë°©ë²•**
   - êµ¬ì²´ì ì´ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ 3-5ê°€ì§€ ë°©ë²•
   - ë§¤ì¼ ë˜ëŠ” ë§¤ì£¼ í•  ìˆ˜ ìˆëŠ” í™œë™

3. **ì¶”ì²œ í•™ìŠµ ìë£Œ**
   - ì˜ì–´ í•™ìŠµ ì•±, ì±…, ì›¹ì‚¬ì´íŠ¸ ë“±

4. **í•™ìŠµ ì‹œê°„ ë° í™˜ê²½ ê´€ë¦¬**
   - íš¨ê³¼ì ì¸ í•™ìŠµ ì‹œê°„ëŒ€
   - ì§‘ì¤‘ë ¥ì„ ë†’ì´ëŠ” í™˜ê²½ ì¡°ì„±ë²•

ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ì‹¤ì²œ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ë¶€ëª¨ ìƒë‹´ ì „ë¬¸ êµìœ¡ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": advice_prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )

            return response.choices[0].message.content

        except Exception as e:
            return f"í•™ìŠµ ì¡°ì–¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    def _get_attendance_status(self, student_id: str) -> str:
        """ì¶œì„ ë° ìˆ™ì œ í˜„í™© ì¡°íšŒ"""
        try:
            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text="ì¶œì„ë¥ , ìˆ™ì œ ì™„ë£Œìœ¨, í•™ìŠµ íƒœë„",
                use_vector_search=False  # ì§ì ‘ ê·¸ë˜í”„ ì¡°íšŒ
            )

            return f"""**ì¶œì„ ë° í•™ìŠµ íƒœë„ í˜„í™©**

{context}

**í•™ë¶€ëª¨ë‹˜ê»˜ ì•ˆë‚´:**
- ì •ê¸°ì ì¸ ì¶œì„ì´ í•™ìŠµ ì„±ê³¼ì— ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤
- ìˆ™ì œëŠ” ìˆ˜ì—… ë‚´ìš© ë³µìŠµ ë° ì´í•´ë„ í™•ì¸ì„ ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤
- í•™ìŠµ íƒœë„ëŠ” ì¥ê¸°ì ì¸ ì˜ì–´ ì‹¤ë ¥ í–¥ìƒì˜ í•µì‹¬ì…ë‹ˆë‹¤
"""
        except Exception as e:
            return f"ì¶œì„ í˜„í™© ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    def _recommend_improvement_areas(self, student_id: str, priority: str = "urgent") -> str:
        """ê°œì„  ì˜ì—­ ì¶”ì²œ"""
        try:
            # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ì— ë”°ë¼ ì¿¼ë¦¬ ì¡°ì •
            if priority == "urgent":
                query = "ê°€ì¥ ì‹œê¸‰í•˜ê²Œ ê°œì„ ì´ í•„ìš”í•œ ì•½ì  ì˜ì—­"
            elif priority == "balanced":
                query = "ê· í˜• ì¡íŒ í•™ìŠµì„ ìœ„í•œ ì „ì²´ ì˜ì—­ ê°œì„  ê³„íš"
            else:  # strength
                query = "ê°•ì ì„ ë”ìš± ê°•í™”í•˜ê¸° ìœ„í•œ ì‹¬í™” í•™ìŠµ ë°©ì•ˆ"

            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text=query,
                use_vector_search=True
            )

            # ê°œì„  ê³„íš ìƒì„± (GPT-4o ì‚¬ìš©)
            plan_prompt = f"""ë‹¹ì‹ ì€ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í•™ìƒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ë¶€ëª¨ë‹˜ê»˜ êµ¬ì²´ì ì¸ ê°œì„  ê³„íšì„ ì œì‹œí•´ì£¼ì„¸ìš”.

**í•™ìƒ ì •ë³´:**
{context}

**ìš°ì„ ìˆœìœ„:**
{priority} ({'ì‹œê¸‰í•œ ì•½ì  ê°œì„ ' if priority == 'urgent' else 'ê· í˜• ì¡íŒ í•™ìŠµ' if priority == 'balanced' else 'ê°•ì  ê°•í™”'})

**ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:**

1. **ìš°ì„  ê°œì„  ì˜ì—­ (1-2ê°œ)**
   - ì˜ì—­ëª…ê³¼ í˜„ì¬ ìˆ˜ì¤€
   - ê°œì„ ì´ í•„ìš”í•œ ì´ìœ 

2. **4ì£¼ í•™ìŠµ ê³„íš**
   - ì£¼ì°¨ë³„ í•™ìŠµ ëª©í‘œ
   - êµ¬ì²´ì ì¸ í•™ìŠµ í™œë™ (ë§¤ì¼ 10-15ë¶„)

3. **ì¤‘ê°„ ì ê²€ ë°©ë²•**
   - 2ì£¼ í›„ í™•ì¸í•  ì‚¬í•­
   - ê°œì„  ì—¬ë¶€ íŒë‹¨ ê¸°ì¤€

4. **í•™ì›ê³¼ ê°€ì •ì˜ ì—­í• **
   - í•™ì›ì—ì„œ ì œê³µí•˜ëŠ” ê²ƒ
   - ê°€ì •ì—ì„œ ë„ì™€ì£¼ì‹¤ ê²ƒ

ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ì‹¤ì²œ ê°€ëŠ¥í•œ ê³„íšì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ë¶€ëª¨ ìƒë‹´ ì „ë¬¸ êµìœ¡ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": plan_prompt}
                ],
                temperature=0.7,
                max_tokens=1200
            )

            return response.choices[0].message.content

        except Exception as e:
            return f"ê°œì„  ê³„íš ìƒì„± ì‹¤íŒ¨: {str(e)}"

    def _execute_function(self, function_name: str, arguments: Dict[str, Any]) -> str:
        """Function ì‹¤í–‰"""
        print(f"ğŸ”§ FUNCTION CALLED: {function_name}")
        print(f"ğŸ“‹ ARGUMENTS: {json.dumps(arguments, ensure_ascii=False)}")

        if function_name == "get_child_info":
            # ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•´ í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì „ë‹¬
            return self._get_child_info(query_text=self.current_user_message, **arguments)
        elif function_name == "analyze_performance":
            return self._analyze_performance(**arguments)
        elif function_name == "get_study_advice":
            return self._get_study_advice(**arguments)
        elif function_name == "get_attendance_status":
            return self._get_attendance_status(**arguments)
        elif function_name == "recommend_improvement_areas":
            return self._recommend_improvement_areas(**arguments)
        else:
            return f"Unknown function: {function_name}"

    def chat(
        self,
        student_id: str,
        message: str,
        chat_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        í•™ë¶€ëª¨ì™€ ì±„íŒ… (Function Calling)

        Args:
            student_id: ìë…€ì˜ í•™ìƒ ID
            message: í•™ë¶€ëª¨ì˜ ë©”ì‹œì§€
            chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒ)

        Returns:
            Dict with 'message' and 'model_info'
        """
        try:
            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ (ë²¡í„° ê²€ìƒ‰ìš©)
            self.current_user_message = message

            used_models = []  # Track models used

            # PromptManagerë¥¼ ì‚¬ìš©í•´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            system_prompt = PromptManager.get_system_prompt(
                role="parent",
                model="gpt-4.1-mini",
                context={"student_id": student_id}
            )

            # ë©”ì‹œì§€ êµ¬ì„± (ë©€í‹°í„´ ì§€ì›)
            messages = [{"role": "system", "content": system_prompt}]

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
            if chat_history:
                messages.extend(chat_history)

            # ìµœì‹  ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            messages.append({"role": "user", "content": message})

            # gpt-4.1-minië¡œ function calling (ë¹ ë¥¸ ì¸í…”ë¦¬ì „ìŠ¤ ëª¨ë¸)
            used_models.append("gpt-4.1-mini")
            response = self.client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=messages,
                tools=self.functions,
                tool_choice="auto",  # ìë™ìœ¼ë¡œ ë„êµ¬ ì„ íƒ
                temperature=0.7
            )

            assistant_message = response.choices[0].message

            # Function í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
            if assistant_message.tool_calls:
                # Function ì‹¤í–‰
                messages.append(assistant_message)

                for tool_call in assistant_message.tool_calls:
                    function_name = tool_call.function.name
                    arguments = json.loads(tool_call.function.arguments)

                    print(f"ğŸ”§ Function Call: {function_name}({arguments})")

                    # Track special models used in functions
                    if function_name in ["get_study_advice", "recommend_improvement_areas"]:
                        used_models.append("gpt-4o")

                    # Function ì‹¤í–‰
                    function_response = self._execute_function(function_name, arguments)

                    # Function ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": function_response
                    })

                # Function ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
                final_response = self.client.chat.completions.create(
                    model="gpt-4.1-mini",
                    messages=messages,
                    temperature=0.7
                )

                response_content = final_response.choices[0].message.content
                return {
                    "message": response_content,
                    "model_info": {
                        "primary": "gpt-4.1-mini",
                        "all_used": list(set(used_models))
                    }
                }
            else:
                # Function í˜¸ì¶œ ì—†ì´ ì§ì ‘ ì‘ë‹µ
                return {
                    "message": assistant_message.content,
                    "model_info": {
                        "primary": "gpt-4.1-mini",
                        "all_used": ["gpt-4.1-mini"]
                    }
                }

        except Exception as e:
            return {
                "message": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "model_info": {"primary": "error", "all_used": []}
            }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_parent_agent_service = None


def get_parent_agent_service() -> ParentAgentService:
    """Parent Agent ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _parent_agent_service
    if _parent_agent_service is None:
        _parent_agent_service = ParentAgentService()
    return _parent_agent_service
