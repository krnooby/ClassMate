# -*- coding: utf-8 -*-
"""
Parent Agent Service
OpenAI Function Calling ê¸°ë°˜ í•™ë¶€ëª¨ ìƒë‹´ ì‹œìŠ¤í…œ
"""
from __future__ import annotations
import os
import json
import re
import random
from typing import List, Dict, Any, Optional
from openai import OpenAI
from shared.services import (
    get_graph_rag_service,
    get_dictionary_service,
    get_news_service,
    get_text_analysis_service,
    get_grammar_check_service
)
from shared.services.youtube_service import get_youtube_service
from shared.prompts import PromptManager
from shared.services.tts_service import get_tts_service


# ANSI ìƒ‰ìƒ ì½”ë“œ
class Colors:
    RESET = '\033[0m'
    BOLD = '\033[1m'

    # ìƒ‰ìƒ
    BLACK = '\033[30m'
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    WHITE = '\033[37m'

    # ë°°ê²½ìƒ‰
    BG_BLACK = '\033[40m'
    BG_RED = '\033[41m'
    BG_GREEN = '\033[42m'
    BG_YELLOW = '\033[43m'
    BG_BLUE = '\033[44m'
    BG_MAGENTA = '\033[45m'
    BG_CYAN = '\033[46m'
    BG_WHITE = '\033[47m'


def print_box(title: str, content: str, color: str = Colors.CYAN):
    """ë°•ìŠ¤ í˜•íƒœë¡œ ë¡œê·¸ ì¶œë ¥"""
    width = 80
    print(f"\n{color}{'='*width}{Colors.RESET}")
    print(f"{color}{Colors.BOLD}{title.center(width)}{Colors.RESET}")
    print(f"{color}{'='*width}{Colors.RESET}")
    for line in content.split('\n'):
        if line.strip():
            print(f"{color}  {line}{Colors.RESET}")
    print(f"{color}{'='*width}{Colors.RESET}\n")


def print_section(emoji: str, title: str, content: str, color: str = Colors.BLUE):
    """ì„¹ì…˜ í˜•íƒœë¡œ ë¡œê·¸ ì¶œë ¥"""
    print(f"\n{color}{Colors.BOLD}{emoji} {title}{Colors.RESET}")
    print(f"{color}{'-'*60}{Colors.RESET}")
    for line in content.split('\n'):
        if line.strip():
            print(f"{color}  {line}{Colors.RESET}")
    print()


class ParentAgentService:
    """í•™ë¶€ëª¨ ë§ì¶¤ Function Calling ì—ì´ì „íŠ¸"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=self.openai_api_key)
        self.graph_rag_service = get_graph_rag_service()

        # Current user message for vector search context
        self.current_user_message = ""

        # Current session ID for audio tracking
        self.current_session_id = None

        # Function definitions
        self.functions = self._create_functions()

    def _route_query(self, message: str, student_id: str) -> str:
        """
        ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ
        Returns: "intelligence" (gpt-4.1-mini) or "reasoning" (o4-mini/o3)
        """
        routing_prompt = f'''Analyze this parent's question and choose the appropriate model:

**intelligence** (gpt-4.1-mini) - Fast, cost-effective for:
- Simple information lookup (ì„±ì  ì¡°íšŒ, ì¶œì„ í™•ì¸, ê¸°ë³¸ ì •ë³´)
- Direct questions with single answer (ëª‡ ì ?, ëˆ„ê°€?, ì–¸ì œ?)
- Greetings and small talk (ì•ˆë…•í•˜ì„¸ìš”, ì˜ ì§€ë‚´?)
- Function calls only (DB ì¡°íšŒë§Œ í•„ìš”)
- Basic status requests
Examples: "ìš°ë¦¬ ì•„ì´ ì„±ì ì´ ì–´ë•Œ?", "ì¶œì„ë¥ ì€?", "ê°•ì ì´ ë­ì•¼?", "ìµœê·¼ í‘¼ ë¬¸ì œëŠ”?", "ì•ˆë…•í•˜ì„¸ìš”"

**reasoning** (o4-mini) - Deep thinking for:
- Multi-step analysis (ì—¬ëŸ¬ ë‹¨ê³„ ë¶„ì„ í•„ìš”)
- Comparative reasoning (ë¹„êµ ë¶„ì„, íŠ¸ë Œë“œ íŒŒì•…)
- Strategic planning (í•™ìŠµ ê³„íš, ê°œì„  ë°©ì•ˆ ìˆ˜ë¦½)
- Cause-effect relationships (ì›ì¸ ë¶„ì„ â†’ í•´ê²°ì±…)
- Synthesis of multiple data points (ì¢…í•©ì  íŒë‹¨)
Examples: "ì•½ì ì„ ë¶„ì„í•˜ê³  4ì£¼ ê³„íš ì„¸ì›Œì¤˜", "ë‹¤ë¥¸ ì•„ì´ë“¤ê³¼ ë¹„êµí•´ì„œ ì–´ë–¤ ë¶€ë¶„ì„ ê°œì„ í•´ì•¼ í• ê¹Œ?", "ì™œ ì„±ì ì´ ë–¨ì–´ì¡Œê³  ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?", "ë“£ê¸°ì™€ ë¬¸ë²• ì¤‘ ë­˜ ë¨¼ì € ê³µë¶€í•´ì•¼ í• ê¹Œ?"

Question: "{message}"

Respond with ONLY "intelligence" or "reasoning".'''

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # Cheap, fast router
                messages=[{"role": "user", "content": routing_prompt}],
                max_tokens=10,
                temperature=0
            )
            decision = response.choices[0].message.content.strip().lower()

            # ì‹œê°ì  ë¡œê¹…
            if decision == "intelligence":
                print_section(
                    "ğŸ§ ",
                    "ROUTING DECISION",
                    f"Query: {message[:50]}...\n"
                    f"Model: INTELLIGENCE (gpt-4.1-mini)\n"
                    f"Reason: Fast function calling for simple tasks",
                    Colors.CYAN
                )
            else:
                print_section(
                    "ğŸ§ ",
                    "ROUTING DECISION",
                    f"Query: {message[:50]}...\n"
                    f"Model: REASONING (o4-mini)\n"
                    f"Reason: Deep thinking required for complex analysis",
                    Colors.MAGENTA
                )

            return decision if decision in ["intelligence", "reasoning"] else "intelligence"
        except Exception as e:
            print(f"{Colors.RED}âš ï¸  Routing failed: {e}, defaulting to intelligence{Colors.RESET}")
            return "intelligence"  # Default fallback

    def _check_response_quality(self, response_text: str) -> bool:
        """
        Check if o4-mini response quality is acceptable
        Returns: True if good, False if needs o3 fallback
        """
        # Bad indicators: too short, generic, error patterns
        if len(response_text) < 50:
            return False
        if "ì£„ì†¡í•©ë‹ˆë‹¤" in response_text and "ì˜¤ë¥˜" in response_text:
            return False
        if response_text.count("...") > 3:  # Too many ellipses = incomplete
            return False
        return True

    def _needs_react(self, message: str) -> bool:
        """
        ReAct ëª¨ë“œê°€ í•„ìš”í•œ ë³µì¡í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨

        ë³µì¡í•œ ì§ˆë¬¸ íŒ¨í„´:
        1. ì—°ê²°ì–´ê°€ ìˆëŠ” ë‹¤ë‹¨ê³„ ì‘ì—…: "~í•˜ê³  ~í•´ì¤˜", "~ì°¾ì•„ì„œ ~í•´ì¤˜"
        2. ìˆœì°¨ì  ì§€ì‹œ: "ë¨¼ì € ~ ê·¸ë‹¤ìŒ ~"
        3. ë™ì‚¬ 3ê°œ ì´ìƒ (ì—¬ëŸ¬ ì‘ì—…)
        """
        import re

        # íŒ¨í„´ 1: ì—°ê²°ì–´ ("í•˜ê³ ", "ì°¾ì•„ì„œ", "í™•ì¸í•˜ê³ " ë“±)
        multi_task_keywords = ['í•˜ê³ ', 'ê·¸ë¦¬ê³ ', 'ì°¾ì•„ì„œ', 'í™•ì¸í•˜ê³ ', 'ì¡°íšŒí•˜ê³ ', 'ì•Œë ¤ì£¼ê³ ', 'ë¶„ì„í•˜ê³ ', 'ì¶”ì²œí•´']
        for keyword in multi_task_keywords:
            if keyword in message and ('í•´ì¤˜' in message or 'ì£¼ì„¸ìš”' in message or 'ì¤˜' in message):
                return True

        # íŒ¨í„´ 2: "ë¨¼ì €...ê·¸ë‹¤ìŒ" ë˜ëŠ” "ë¨¼ì €...ê·¸ë¦¬ê³ "
        if 'ë¨¼ì €' in message and ('ê·¸ë‹¤ìŒ' in message or 'ê·¸ë¦¬ê³ ' in message):
            return True

        # íŒ¨í„´ 3: ë™ì‚¬ 3ê°œ ì´ìƒ
        action_verbs = ['ì°¾', 'ë¶„ì„', 'ì¶”ì²œ', 'í™•ì¸', 'ì¡°íšŒ', 'ê²€ìƒ‰', 'ë¹„êµ', 'ìƒì„±', 'ì„¤ëª…', 'ì•Œë ¤', 'ë³´ì—¬', 'ì„¸ì›Œ']
        verb_count = sum(1 for verb in action_verbs if verb in message)
        if verb_count >= 3:
            return True

        return False

    def _react_chat(
        self,
        student_id: str,
        message: str,
        chat_history: Optional[List[Dict[str, str]]] = None,
        max_steps: int = 5
    ) -> Dict[str, Any]:
        """
        ReAct (Reasoning + Acting) ëª¨ë“œë¡œ ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì‘ì—… ì²˜ë¦¬

        Args:
            student_id: í•™ìƒ ID (í•™ë¶€ëª¨ì˜ ìë…€)
            message: í•™ë¶€ëª¨ì˜ ë©”ì‹œì§€
            chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡
            max_steps: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜

        Returns:
            Dict with 'message' and 'model_info'
        """
        print(f"\n{'='*60}")
        print(f"ğŸ”„ ReAct Mode Activated (Parent)")
        print(f"Query: {message}")
        print(f"{'='*60}\n")

        # System prompt
        system_prompt = PromptManager.get_system_prompt(
            role="parent",
            model="o4-mini",
            context={"student_id": student_id}
        )

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]

        if chat_history:
            messages.extend(chat_history)

        messages.append({"role": "user", "content": message})

        used_models = ["gpt-4o-mini (router)", "o4-mini (ReAct)"]

        # ReAct ë£¨í”„
        for step in range(1, max_steps + 1):
            print(f"\n--- ReAct Step {step}/{max_steps} ---")

            # LLM í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model="o4-mini",
                messages=messages,
                tools=self.functions,
                tool_choice="auto",
                max_completion_tokens=10000
            )

            assistant_message = response.choices[0].message

            # Thought ì¶œë ¥ (LLMì˜ ì‚¬ê³  ê³¼ì •)
            if assistant_message.content:
                print(f"ğŸ’­ Thought: {assistant_message.content[:200]}...")

            # Function í˜¸ì¶œì´ ìˆìœ¼ë©´ ì‹¤í–‰
            if assistant_message.tool_calls:
                messages.append(assistant_message)

                for tool_call in assistant_message.tool_calls:
                    function_name = tool_call.function.name
                    arguments = json.loads(tool_call.function.arguments)

                    print(f"ğŸ”§ Action: {function_name}({arguments})")

                    # Function ì‹¤í–‰
                    result = self._execute_function(function_name, arguments)

                    print(f"ğŸ“Š Observation: {result[:200]}...")

                    # Observationì„ ë©”ì‹œì§€ì— ì¶”ê°€
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result
                    })
            else:
                # Final answer (Function í˜¸ì¶œ ì—†ìŒ)
                print(f"âœ… Final Answer Reached")

                content = assistant_message.content or "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                # Quick reply íŒŒì‹±
                parsed = self._parse_quick_reply(content, used_models)

                return parsed

        # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì´ˆê³¼
        print(f"âš ï¸  Max steps ({max_steps}) reached")

        # ë§ˆì§€ë§‰ ì‘ë‹µ ìƒì„±
        final_response = self.client.chat.completions.create(
            model="o4-mini",
            messages=messages,
            max_completion_tokens=10000
        )

        content = final_response.choices[0].message.content or "ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”."

        parsed = self._parse_quick_reply(content, used_models)

        return parsed

    def _create_functions(self) -> List[Dict[str, Any]]:
        """OpenAI Function Callingìš© function ì •ì˜"""
        return [
            {
                "type": "function",
                "function": {
                    "name": "get_child_info",
                    "description": "ìë…€ì˜ ìƒì„¸ í•™ìŠµ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì´ë¦„, í•™ë…„, CEFR ë ˆë²¨, ê°•ì , ì•½ì , ì˜ì—­ë³„ ì ìˆ˜, ì¶œì„ë¥ , ìˆ™ì œ ì™„ë£Œìœ¨, í•™ìŠµ íƒœë„, ë°˜ ì •ë³´ ë“±)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "ìë…€ì˜ í•™ìƒ ID (ì˜ˆ: S-01)"
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "analyze_performance",
                    "description": "ìë…€ì˜ ì„±ì  ë¶„ì„ ë° í•™ìŠµ ì¶”ì´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì˜ì—­ë³„ ê°•ì•½ì , ë˜ë˜ ë¹„êµ, ê°œì„  ì¶”ì´)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "ìë…€ì˜ í•™ìƒ ID"
                            },
                            "focus_area": {
                                "type": "string",
                                "description": "ì§‘ì¤‘ ë¶„ì„í•  ì˜ì—­ (ë…í•´, ë¬¸ë²•, ì–´íœ˜, ë“£ê¸°, ì“°ê¸° ì¤‘ í•˜ë‚˜, ë˜ëŠ” nullì¼ ê²½ìš° ì „ì²´ ë¶„ì„)",
                                "enum": ["ë…í•´", "ë¬¸ë²•", "ì–´íœ˜", "ë“£ê¸°", "ì“°ê¸°", None]
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_study_advice",
                    "description": "ìë…€ì˜ ì•½ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ ë§ì¶¤í˜• í•™ìŠµ ì¡°ì–¸ ë° ê°€ì • í•™ìŠµ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "ìë…€ì˜ í•™ìƒ ID"
                            },
                            "area": {
                                "type": "string",
                                "description": "ì¡°ì–¸ì´ í•„ìš”í•œ ì˜ì—­ (ë…í•´, ë¬¸ë²•, ì–´íœ˜, ë“£ê¸°, ì“°ê¸° ì¤‘ í•˜ë‚˜, ë˜ëŠ” nullì¼ ê²½ìš° ì „ì²´ ì•½ì  ê¸°ë°˜)",
                                "enum": ["ë…í•´", "ë¬¸ë²•", "ì–´íœ˜", "ë“£ê¸°", "ì“°ê¸°", None]
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_attendance_status",
                    "description": "ìë…€ì˜ ì¶œì„ í˜„í™©, ìˆ™ì œ ì™„ë£Œìœ¨, í•™ìŠµ íƒœë„ ë“± ì¼ì¼ í•™ìŠµ ê¸°ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "ìë…€ì˜ í•™ìƒ ID"
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "recommend_improvement_areas",
                    "description": "ìë…€ê°€ ì§‘ì¤‘ì ìœ¼ë¡œ ê°œì„ í•´ì•¼ í•  ì˜ì—­ê³¼ êµ¬ì²´ì ì¸ í•™ìŠµ ê³„íšì„ ì¶”ì²œí•©ë‹ˆë‹¤",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "ìë…€ì˜ í•™ìƒ ID"
                            },
                            "priority": {
                                "type": "string",
                                "description": "ìš°ì„ ìˆœìœ„ ê¸°ì¤€ (urgent: ì‹œê¸‰í•œ ì•½ì , balanced: ê· í˜• ì¡íŒ í•™ìŠµ, strength: ê°•ì  ê°•í™”)",
                                "enum": ["urgent", "balanced", "strength"]
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "generate_problem",
                    "description": "AIê°€ ìƒˆë¡œìš´ ì˜ì–´ ë¬¸ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ê³ í’ˆì§ˆ). ë“£ê¸° ë¬¸ì œëŠ” ìë™ìœ¼ë¡œ ëŒ€í™” ìŒì„±ê³¼ [AUDIO], [SPEAKERS] íƒœê·¸ê°€ í¬í•¨ë©ë‹ˆë‹¤. difficultyë¥¼ ìƒëµí•˜ë©´ í•™ìƒì˜ CEFR ë ˆë²¨ì— ë§ì¶° ìë™ ìƒì„±ë©ë‹ˆë‹¤.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "í•™ìƒ ID (CEFR ë ˆë²¨ ìë™ ì¡°íšŒìš©)"
                            },
                            "area": {
                                "type": "string",
                                "description": "ë¬¸ì œ ì˜ì—­: 'ë“£ê¸°' (listening), 'ë…í•´' (reading), 'ë¬¸ë²•' (grammar), 'ì–´íœ˜' (vocabulary), 'ì“°ê¸°' (writing)"
                            },
                            "difficulty": {
                                "type": "string",
                                "description": "ë‚œì´ë„ CEFR ë ˆë²¨ (A1, A2, B1, B2, C1, C2). ìƒëµ ì‹œ í•™ìƒì˜ í˜„ì¬ ë ˆë²¨ ì‚¬ìš©"
                            },
                            "topic": {
                                "type": "string",
                                "description": "ì£¼ì œ - ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì£¼ì œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: ì—¬í–‰, ì‡¼í•‘, ë ˆìŠ¤í† ë‘ ì˜ˆì•½, ë³‘ì› ì˜ˆì•½, ë„ì„œê´€, ì˜í™”ê´€, ìš´ë™, ì·¨ë¯¸, í•™êµ ìƒí™œ, ì¹œêµ¬ ëª¨ì„, ê°€ì¡± í–‰ì‚¬, íœ´ê°€ ê³„íš, ë´‰ì‚¬í™œë™ ë“±. ì–¸ê¸‰ì´ ì—†ìœ¼ë©´ ë‹¤ì–‘í•œ ì£¼ì œ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”."
                            },
                            "num_speakers": {
                                "type": "integer",
                                "description": "ë“£ê¸° ë¬¸ì œì˜ í™”ì ìˆ˜ (2, 3, 4 ë“±). ì‚¬ìš©ìê°€ '3ëª…', 'ì—¬ëŸ¬ ëª…', 'ì„¸ ëª…' ë“±ì„ ì–¸ê¸‰í•˜ë©´ í•´ë‹¹ ìˆ«ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ê¸°ë³¸ê°’ì€ 2ëª…ì…ë‹ˆë‹¤."
                            }
                        },
                        "required": ["student_id", "area"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "lookup_word",
                    "description": "ì˜ì–´ ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë°œìŒ, ì •ì˜, ì˜ˆë¬¸, ë™ì˜ì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤ (ìë…€ í•™ìŠµ ì§€ì›ìš©)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "word": {
                                "type": "string",
                                "description": "ê²€ìƒ‰í•  ì˜ì–´ ë‹¨ì–´ (ì˜ˆ: confident, beautiful, education)"
                            }
                        },
                        "required": ["word"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "fetch_news",
                    "description": "ì˜ì–´ í•™ìŠµìš© ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ìë…€ì™€ í•¨ê»˜ ì½ì„ ìˆ˜ ìˆëŠ” ì˜ì–´ ë‰´ìŠ¤)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "category": {
                                "type": "string",
                                "description": "ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬: general(ì¼ë°˜), sports(ìŠ¤í¬ì¸ ), technology(ê¸°ìˆ ), health(ê±´ê°•), science(ê³¼í•™), entertainment(ì—”í„°)",
                                "enum": ["general", "sports", "technology", "health", "science", "entertainment"],
                                "default": "general"
                            },
                            "page_size": {
                                "type": "integer",
                                "description": "ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (1-10, ê¸°ë³¸ê°’ 3)",
                                "default": 3,
                                "minimum": 1,
                                "maximum": 10
                            }
                        },
                        "required": []
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "analyze_text_difficulty",
                    "description": "ì˜ì–´ í…ìŠ¤íŠ¸ì˜ ë‚œì´ë„(CEFR ë ˆë²¨)ì™€ ê°€ë…ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤ (ìë…€ê°€ ì½ëŠ” ê¸€ì˜ ìˆ˜ì¤€ íŒŒì•…ìš©)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "text": {
                                "type": "string",
                                "description": "ë¶„ì„í•  ì˜ì–´ í…ìŠ¤íŠ¸ (ìµœì†Œ 1ë¬¸ì¥ ì´ìƒ)"
                            }
                        },
                        "required": ["text"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "check_grammar",
                    "description": "ì˜ì–´ ë¬¸ì¥ì˜ ë¬¸ë²• ì˜¤ë¥˜ë¥¼ ìë™ìœ¼ë¡œ ê²€ì‚¬í•˜ê³  ìˆ˜ì • ì œì•ˆì„ ì œê³µí•©ë‹ˆë‹¤ (ìë…€ í•™ìŠµ ì§€ì›ìš©)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "text": {
                                "type": "string",
                                "description": "ê²€ì‚¬í•  ì˜ì–´ ë¬¸ì¥ ë˜ëŠ” ë‹¨ë½"
                            }
                        },
                        "required": ["text"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "search_youtube",
                    "description": "ìë…€ì˜ ì˜ì–´ í•™ìŠµì— ë„ì›€ë˜ëŠ” YouTube ì˜ìƒ/ì±„ë„ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. CEFR ë ˆë²¨ê³¼ ê´€ì‹¬ ì£¼ì œì— ë§ì¶° êµìœ¡ìš© ì½˜í…ì¸ ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "í•™ìƒ ID (CEFR ë ˆë²¨ ìë™ ì¡°íšŒìš©)"
                            },
                            "search_type": {
                                "type": "string",
                                "description": "ê²€ìƒ‰ íƒ€ì…: 'video' (ì˜ìƒ ê²€ìƒ‰) ë˜ëŠ” 'channel' (ì±„ë„ ê²€ìƒ‰)",
                                "enum": ["video", "channel"],
                                "default": "video"
                            },
                            "topic": {
                                "type": "string",
                                "description": "ê²€ìƒ‰ ì£¼ì œ (ì˜ˆ: 'English listening practice', 'kids vocabulary', 'phonics songs'). í•™ìƒì˜ CEFR ë ˆë²¨ê³¼ í¥ë¯¸ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
                            },
                            "max_results": {
                                "type": "integer",
                                "description": "ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜ (1-10, ê¸°ë³¸ê°’ 5)",
                                "default": 5,
                                "minimum": 1,
                                "maximum": 10
                            }
                        },
                        "required": ["student_id", "topic"]
                    }
                }
            }
        ]

    def _get_child_info(self, student_id: str, query_text: str = "í•™ìƒ ì •ë³´ ì¡°íšŒ") -> str:
        """ìë…€ ì •ë³´ ì¡°íšŒ ì‹¤í–‰ (ë²¡í„° ê²€ìƒ‰ í¬í•¨)"""
        try:
            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text=query_text,
                use_vector_search=True
            )

            # í•™ë¶€ëª¨ ê´€ì ì—ì„œ ì •ë³´ ì¬êµ¬ì„±
            if context:
                import re
                # ì´ë¦„ ì¶”ì¶œ
                name_match = re.search(r'ì´ë¦„: (.+)', context)
                student_name = name_match.group(1) if name_match else "ìë…€"

                # CEFR ë ˆë²¨ ì¶”ì¶œ
                cefr_match = re.search(r'CEFR ë ˆë²¨: ([A-C][1-2])', context)
                cefr_level = cefr_match.group(1) if cefr_match else "N/A"

                return f"""**{student_name} í•™ìƒì˜ í•™ìŠµ í˜„í™© (í•™ë¶€ëª¨ë‹˜ ê´€ì ):**

{context}

**í•™ë¶€ëª¨ë‹˜ê»˜ ì•ˆë‚´ ì‚¬í•­:**
- í˜„ì¬ ìë…€ì˜ CEFR ë ˆë²¨ì€ {cefr_level}ì…ë‹ˆë‹¤
- ì •ê¸°ì ì¸ ì¶œì„ê³¼ ìˆ™ì œ ì™„ë£Œê°€ ì„±ì  í–¥ìƒì— ì¤‘ìš”í•©ë‹ˆë‹¤
- ê°€ì •ì—ì„œë„ ê¾¸ì¤€í•œ í•™ìŠµ ìŠµê´€ í˜•ì„±ì´ í•„ìš”í•©ë‹ˆë‹¤
"""
            return context
        except Exception as e:
            return f"ìë…€ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    def _analyze_performance(self, student_id: str, focus_area: Optional[str] = None) -> str:
        """ì„±ì  ë¶„ì„ ì‹¤í–‰"""
        try:
            # GraphRAGë¥¼ í†µí•´ ì„±ì  ì •ë³´ ì¡°íšŒ
            query = f"ìë…€ì˜ {focus_area} ì„±ì  ë¶„ì„" if focus_area else "ìë…€ì˜ ì „ì²´ ì„±ì  ë¶„ì„ ë° ë˜ë˜ ë¹„êµ"
            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text=query,
                use_vector_search=True
            )

            # ì„±ì  ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
            analysis = f"""**ì„±ì  ë¶„ì„ ë¦¬í¬íŠ¸**

{context}

**ë˜ë˜ ë¹„êµ ë¶„ì„:**
- ê°™ì€ ë°˜ í•™ìƒë“¤ê³¼ ë¹„êµí•˜ì—¬ ìƒìœ„ê¶Œ/ì¤‘ìœ„ê¶Œ/í•˜ìœ„ê¶Œ íŒŒì•…
- ì˜ì—­ë³„ ê°•ì•½ì  ë¶„ì„

**ê°œì„  ì¶”ì´:**
- ìµœê·¼ í•™ìŠµ í™œë™ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ì„±ì¥ ê°€ëŠ¥ì„± ì˜ˆì¸¡
- ê¾¸ì¤€í•œ í•™ìŠµì´ í•„ìš”í•œ ì˜ì—­ ì‹ë³„
"""
            return analysis

        except Exception as e:
            return f"ì„±ì  ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    def _get_study_advice(self, student_id: str, area: Optional[str] = None) -> str:
        """í•™ìŠµ ì¡°ì–¸ ì œê³µ"""
        try:
            # GraphRAGë¥¼ í†µí•´ ì•½ì  ì˜ì—­ íŒŒì•…
            query = f"{area} ì˜ì—­ ê°œì„  ë°©ì•ˆ" if area else "ì „ì²´ ì•½ì  ê°œì„  ë°©ì•ˆ"
            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text=query,
                use_vector_search=True
            )

            # ëª¨ë¸ ì„ íƒ: íŠ¹ì • ì˜ì—­ ì¡°ì–¸ì€ ê°„ë‹¨ â†’ gpt-4.1-mini, ì „ì²´ ì˜ì—­ì€ ì¶”ë¡  í•„ìš” â†’ o4-mini
            model = "gpt-4.1-mini" if area else "o4-mini"
            print(f"ğŸ“Š í•™ìŠµ ì¡°ì–¸ ìƒì„± ëª¨ë¸: {model}")

            # í•™ìŠµ ì¡°ì–¸ ìƒì„±
            advice_prompt = f"""ë‹¹ì‹ ì€ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í•™ìƒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ë¶€ëª¨ë‹˜ê»˜ ë§ì¶¤í˜• í•™ìŠµ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.

**í•™ìƒ ì •ë³´:**
{context}

**ì¡°ì–¸ ì˜ì—­:**
{area if area else "ì „ì²´ ì˜ì—­"}

**ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:**

1. **í˜„ì¬ ìƒí™© ìš”ì•½**
   - ìë…€ì˜ ê°•ì ê³¼ ì•½ì  ê°„ëµíˆ ì •ë¦¬

2. **ê°€ì •ì—ì„œ ë„ì™€ì£¼ì‹¤ ìˆ˜ ìˆëŠ” ë°©ë²•**
   - êµ¬ì²´ì ì´ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ 3-5ê°€ì§€ ë°©ë²•
   - ë§¤ì¼ ë˜ëŠ” ë§¤ì£¼ í•  ìˆ˜ ìˆëŠ” í™œë™

3. **ì¶”ì²œ í•™ìŠµ ìë£Œ**
   - ì˜ì–´ í•™ìŠµ ì•±, ì±…, ì›¹ì‚¬ì´íŠ¸ ë“±

4. **í•™ìŠµ ì‹œê°„ ë° í™˜ê²½ ê´€ë¦¬**
   - íš¨ê³¼ì ì¸ í•™ìŠµ ì‹œê°„ëŒ€
   - ì§‘ì¤‘ë ¥ì„ ë†’ì´ëŠ” í™˜ê²½ ì¡°ì„±ë²•

ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ì‹¤ì²œ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""

            # ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ íŒŒë¼ë¯¸í„° ì‚¬ìš©
            if model == "o4-mini":
                response = self.client.chat.completions.create(
                    model="o4-mini",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ë¶€ëª¨ ìƒë‹´ ì „ë¬¸ êµìœ¡ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": advice_prompt}
                    ],
                    max_completion_tokens=3000
                )
            else:
                response = self.client.chat.completions.create(
                    model="gpt-4.1-mini",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ë¶€ëª¨ ìƒë‹´ ì „ë¬¸ êµìœ¡ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": advice_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1000
                )

            return response.choices[0].message.content

        except Exception as e:
            return f"í•™ìŠµ ì¡°ì–¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    def _get_attendance_status(self, student_id: str) -> str:
        """ì¶œì„ ë° ìˆ™ì œ í˜„í™© ì¡°íšŒ"""
        try:
            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text="ì¶œì„ë¥ , ìˆ™ì œ ì™„ë£Œìœ¨, í•™ìŠµ íƒœë„",
                use_vector_search=False  # ì§ì ‘ ê·¸ë˜í”„ ì¡°íšŒ
            )

            return f"""**ì¶œì„ ë° í•™ìŠµ íƒœë„ í˜„í™©**

{context}

**í•™ë¶€ëª¨ë‹˜ê»˜ ì•ˆë‚´:**
- ì •ê¸°ì ì¸ ì¶œì„ì´ í•™ìŠµ ì„±ê³¼ì— ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤
- ìˆ™ì œëŠ” ìˆ˜ì—… ë‚´ìš© ë³µìŠµ ë° ì´í•´ë„ í™•ì¸ì„ ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤
- í•™ìŠµ íƒœë„ëŠ” ì¥ê¸°ì ì¸ ì˜ì–´ ì‹¤ë ¥ í–¥ìƒì˜ í•µì‹¬ì…ë‹ˆë‹¤
"""
        except Exception as e:
            return f"ì¶œì„ í˜„í™© ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    def _recommend_improvement_areas(self, student_id: str, priority: str = "urgent") -> str:
        """ê°œì„  ì˜ì—­ ì¶”ì²œ"""
        try:
            # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ì— ë”°ë¼ ì¿¼ë¦¬ ì¡°ì •
            if priority == "urgent":
                query = "ê°€ì¥ ì‹œê¸‰í•˜ê²Œ ê°œì„ ì´ í•„ìš”í•œ ì•½ì  ì˜ì—­"
            elif priority == "balanced":
                query = "ê· í˜• ì¡íŒ í•™ìŠµì„ ìœ„í•œ ì „ì²´ ì˜ì—­ ê°œì„  ê³„íš"
            else:  # strength
                query = "ê°•ì ì„ ë”ìš± ê°•í™”í•˜ê¸° ìœ„í•œ ì‹¬í™” í•™ìŠµ ë°©ì•ˆ"

            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text=query,
                use_vector_search=True
            )

            # ëª¨ë¸ ì„ íƒ: 4ì£¼ í•™ìŠµ ê³„íš ê°™ì€ ë³µì¡í•œ ì¶”ë¡  â†’ o4-mini
            model = "o4-mini"
            print(f"ğŸ“Š ê°œì„  ê³„íš ìƒì„± ëª¨ë¸: {model}")

            # ê°œì„  ê³„íš ìƒì„±
            plan_prompt = f"""ë‹¹ì‹ ì€ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í•™ìƒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ë¶€ëª¨ë‹˜ê»˜ êµ¬ì²´ì ì¸ ê°œì„  ê³„íšì„ ì œì‹œí•´ì£¼ì„¸ìš”.

**í•™ìƒ ì •ë³´:**
{context}

**ìš°ì„ ìˆœìœ„:**
{priority} ({'ì‹œê¸‰í•œ ì•½ì  ê°œì„ ' if priority == 'urgent' else 'ê· í˜• ì¡íŒ í•™ìŠµ' if priority == 'balanced' else 'ê°•ì  ê°•í™”'})

**ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:**

1. **ìš°ì„  ê°œì„  ì˜ì—­ (1-2ê°œ)**
   - ì˜ì—­ëª…ê³¼ í˜„ì¬ ìˆ˜ì¤€
   - ê°œì„ ì´ í•„ìš”í•œ ì´ìœ 

2. **4ì£¼ í•™ìŠµ ê³„íš**
   - ì£¼ì°¨ë³„ í•™ìŠµ ëª©í‘œ
   - êµ¬ì²´ì ì¸ í•™ìŠµ í™œë™ (ë§¤ì¼ 10-15ë¶„)

3. **ì¤‘ê°„ ì ê²€ ë°©ë²•**
   - 2ì£¼ í›„ í™•ì¸í•  ì‚¬í•­
   - ê°œì„  ì—¬ë¶€ íŒë‹¨ ê¸°ì¤€

4. **í•™ì›ê³¼ ê°€ì •ì˜ ì—­í• **
   - í•™ì›ì—ì„œ ì œê³µí•˜ëŠ” ê²ƒ
   - ê°€ì •ì—ì„œ ë„ì™€ì£¼ì‹¤ ê²ƒ

ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ì‹¤ì²œ ê°€ëŠ¥í•œ ê³„íšì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""

            response = self.client.chat.completions.create(
                model="o4-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ë¶€ëª¨ ìƒë‹´ ì „ë¬¸ êµìœ¡ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": plan_prompt}
                ],
                max_completion_tokens=3000
            )

            return response.choices[0].message.content

        except Exception as e:
            return f"ê°œì„  ê³„íš ìƒì„± ì‹¤íŒ¨: {str(e)}"

    def _generate_problem(self, student_id: str, area: str, difficulty: str = None, topic: str = None, num_speakers: int = 2) -> str:
        """AI ë¬¸ì œ ìƒì„± ì‹¤í–‰ (o4-mini - ë¹ ë¥¸ ì¶”ë¡  ëª¨ë¸)"""
        try:
            # topicì´ ì—†ìœ¼ë©´ ë‹¤ì–‘í•œ ì£¼ì œ ì¤‘ ëœë¤ ì„ íƒ
            if not topic:
                topics = [
                    "ë ˆìŠ¤í† ë‘ ì˜ˆì•½", "ì˜í™”ê´€ ë°©ë¬¸", "ë„ì„œê´€ ì´ìš©", "ì‡¼í•‘", "ë³‘ì› ì˜ˆì•½",
                    "ì—¬í–‰ ê³„íš", "ìš´ë™", "ì·¨ë¯¸ í™œë™", "í•™êµ ìƒí™œ", "ì¹œêµ¬ ëª¨ì„",
                    "ê°€ì¡± í–‰ì‚¬", "íœ´ê°€ ê³„íš", "ë´‰ì‚¬í™œë™", "ë™ì•„ë¦¬ í™œë™", "íŒŒí‹° ì¤€ë¹„"
                ]
                topic = random.choice(topics)
                print(f"ğŸ“Œ ìë™ ì„ íƒëœ ì£¼ì œ: {topic}")

            # difficultyê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ í•™ìƒì˜ CEFR ë ˆë²¨ ì¡°íšŒ
            if not difficulty:
                try:
                    # GraphRAG ì„œë¹„ìŠ¤ë¥¼ í†µí•´ í•™ìƒ ì •ë³´ ì¡°íšŒ
                    student_context = self.graph_rag_service.get_rag_context(
                        student_id=student_id,
                        query_text="í•™ìƒì˜ CEFR ë ˆë²¨ ì•Œë ¤ì¤˜",
                        use_vector_search=False  # ì§ì ‘ ê·¸ë˜í”„ ì¡°íšŒ
                    )

                    # CEFR ë ˆë²¨ ì¶”ì¶œ (A1, A2, B1, B2, C1, C2 ì¤‘ í•˜ë‚˜)
                    cefr_match = re.search(r'\b(A1|A2|B1|B2|C1|C2)\b', student_context, re.IGNORECASE)
                    if cefr_match:
                        difficulty = cefr_match.group(1).upper()
                        print(f"ğŸ“Š í•™ìƒ {student_id}ì˜ CEFR ë ˆë²¨: {difficulty}")
                    else:
                        # ê¸°ë³¸ê°’: B1
                        difficulty = "B1"
                        print(f"âš ï¸  í•™ìƒ {student_id}ì˜ CEFR ë ˆë²¨ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ B1 ì‚¬ìš©")
                except Exception as e:
                    print(f"âš ï¸  CEFR ë ˆë²¨ ì¡°íšŒ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ B1 ì‚¬ìš©")
                    difficulty = "B1"

            # PromptManagerë¥¼ ì‚¬ìš©í•´ ë¬¸ì œ ìƒì„± í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            prompt = PromptManager.get_problem_generation_prompt(
                area=area,
                difficulty=difficulty,
                topic=topic,
                num_speakers=num_speakers,
                model="o4-mini"
            )

            is_listening = area.lower() in ['ë“£ê¸°', 'listening', 'ls']
            print(f"ğŸ¯ ë¬¸ì œ ìƒì„±: area={area}, difficulty={difficulty}, topic={topic}, num_speakers={num_speakers}, is_listening={is_listening}")

            # ë“£ê¸° ë¬¸ì œëŠ” ìµœëŒ€ 2íšŒ ì¬ì‹œë„
            max_attempts = 2 if is_listening else 1

            for attempt in range(max_attempts):
                try:
                    # o4-mini - o3-mini í›„ì†, ë¹ ë¥¸ ì†ë„ + ìš°ìˆ˜í•œ STEM ì„±ëŠ¥
                    # NOTE: o4-mini uses reasoning tokens + output tokens, so we need more tokens for listening problems
                    max_tokens = 10000 if is_listening else 3000
                    response = self.client.chat.completions.create(
                        model="o4-mini",
                        messages=[{"role": "user", "content": prompt}],
                        max_completion_tokens=max_tokens
                    )

                    content = response.choices[0].message.content

                    if is_listening and content:
                        print(f"âœ… o4-mini ë“£ê¸° ë¬¸ì œ ìƒì„± ì™„ë£Œ: {len(content)} ê¸€ì")

                    # ë“£ê¸° ë¬¸ì œ í›„ì²˜ë¦¬
                    if is_listening:
                        content = self._postprocess_listening_problem(content, attempt + 1)
                        # í•™ë¶€ëª¨ìš© ì§€ë„ ê°€ì´ë“œ ì¶”ê°€
                        content = self._add_parent_guidance(content, difficulty, topic)

                    return content

                except Exception as retry_error:
                    if attempt == max_attempts - 1:
                        raise retry_error
                    print(f"âš ï¸  ë“£ê¸° ë¬¸ì œ ìƒì„± ì¬ì‹œë„ ({attempt + 1}/{max_attempts})")
                    continue

        except Exception as e:
            # Fallback to GPT-4o (ë¹ ë¥´ê³  ì•ˆì •ì )
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "You are an expert English language teacher creating high-quality assessment questions."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.8,
                    max_tokens=1500
                )

                content = response.choices[0].message.content

                # ë“£ê¸° ë¬¸ì œ í›„ì²˜ë¦¬
                if is_listening:
                    content = self._postprocess_listening_problem(content, attempt=1)
                    # í•™ë¶€ëª¨ìš© ì§€ë„ ê°€ì´ë“œ ì¶”ê°€
                    content = self._add_parent_guidance(content, difficulty, topic)

                return content

            except Exception as fallback_error:
                return f"ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {str(e)}, Fallback ì‹¤íŒ¨: {str(fallback_error)}"

    def _postprocess_listening_problem(self, content: str, attempt: int) -> str:
        """
        ë“£ê¸° ë¬¸ì œ í›„ì²˜ë¦¬ (ê°•ì œ ê²€ì¦ ë° ìˆ˜ì •)

        1. í•œê¸€ í…ìŠ¤íŠ¸ ì œê±° (TTSì—ì„œ ì½íˆì§€ ì•Šë„ë¡)
        2. [AUDIO]: íŒ¨í„´ í™•ì¸ ë° ì¶”ê°€
        3. [SPEAKERS]: JSON íŒŒì‹± ë° ìë™ ìƒì„±
        4. ëŒ€í™” í˜•ì‹ ê²€ì¦
        5. ì²« ë°œí™”ì— í™”ì ì´ë¦„ ì¶”ê°€
        """
        # ========================================
        # STEP 1: í•œê¸€ í…ìŠ¤íŠ¸ ì œê±° (TTS ìŒì„± ë°©ì§€)
        # ========================================
        # 1. ê´„í˜¸ ì•ˆì˜ í•œê¸€ ë²ˆì—­ ì œê±° (ì˜ˆ: "(ì•ˆë…•, íŒŒí‹°ì— ì˜¬ ê±°ì•¼?)" â†’ "")
        # 2. ì™„ì „íˆ í•œê¸€ë¡œë§Œ ëœ ì„¤ëª… ì¤„ë§Œ ì œê±°
        # 3. ì˜ì–´ ëŒ€í™”ëŠ” ìœ ì§€

        korean_char_pattern = re.compile(r'[\u3131-\u3163\uac00-\ud7a3]')  # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„

        cleaned_lines = []
        for line in content.split('\n'):
            # ê´„í˜¸ ì•ˆì˜ í•œê¸€ ë²ˆì—­ ì œê±° (ì˜ˆ: "Hello! (ì•ˆë…•!) How are you? (ì–´ë–»ê²Œ ì§€ë‚´?)" â†’ "Hello! How are you?")
            cleaned_line = re.sub(r'\([^)]*[\u3131-\u3163\uac00-\ud7a3][^)]*\)', '', line)
            # ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì •ë¦¬
            cleaned_line = re.sub(r'\s+', ' ', cleaned_line).strip()

            # ì™„ì „íˆ í•œê¸€ë¡œë§Œ ëœ ì¤„ë§Œ ì œê±° (ì„¤ëª… ì¤„)
            # ì˜ˆ: "ìŒì„± ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ ë“£ê¸° ì—°ìŠµ", "[ìŒì„± ë‚´ìš© ìš”ì•½]", "- ê³µì› ì²­ì†Œ ë´‰ì‚¬ í™œë™ì´..."
            if re.match(r'^[\s\u3131-\u3163\uac00-\ud7a3\[\]:\-â€¢â€»\(\)]+$', cleaned_line):
                print(f"   ğŸ—‘ï¸  í•œê¸€ ì„¤ëª… ì¤„ ì œê±°: {line[:50]}...")
                continue

            # ë¹ˆ ì¤„ì€ ìœ ì§€ (êµ¬ì¡° ë³´ì¡´)
            cleaned_lines.append(cleaned_line if cleaned_line else line)

        content = '\n'.join(cleaned_lines)
        print(f"   âœ… í•œê¸€ í…ìŠ¤íŠ¸ ì œê±° ì™„ë£Œ (ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ìœ ì§€)")

        lines = content.split('\n')

        # 1. [AUDIO]: íŒ¨í„´ í™•ì¸
        has_audio_tag = any('[AUDIO]:' in line for line in lines)

        # 2. [SPEAKERS]: JSON íŒŒì‹±
        speakers_match = re.search(r'\[SPEAKERS\]:\s*({.*})', content)
        has_speakers_tag = speakers_match is not None

        # Check if voice field exists in existing SPEAKERS JSON
        needs_voice_enhancement = False
        if has_speakers_tag:
            try:
                existing_speakers = json.loads(speakers_match.group(1))
                speakers_list = existing_speakers.get('speakers', [])
                # Check if ANY speaker is missing voice field
                if speakers_list and not all('voice' in s for s in speakers_list):
                    needs_voice_enhancement = True
                    print(f"   âš ï¸  [SPEAKERS]: voice í•„ë“œ ì—†ìŒ â†’ o4-minië¡œ ì¶”ê°€ ì˜ˆì •")
            except:
                pass

        # 3. ëŒ€í™” í˜•ì‹ í™•ì¸ (Name: text íŒ¨í„´)
        dialogue_pattern = re.compile(r'^([A-Z][a-z]+):\s+.+', re.MULTILINE)
        dialogue_matches = dialogue_pattern.findall(content)

        # ì²« ë²ˆì§¸ ëŒ€í™” ì°¾ê¸° (í™”ì ì—†ì´ ì‹œì‘í•˜ëŠ” ê²½ìš°ë„ í¬í•¨)
        # ì˜ˆ: "Hi Jake, have you done..." ê°™ì€ ê²½ìš°
        first_utterance_pattern = re.compile(r'^[A-Z][^:]{10,}', re.MULTILINE)
        first_utterance_matches = first_utterance_pattern.findall(content)

        has_dialogue = len(dialogue_matches) >= 1 or len(first_utterance_matches) >= 1

        print(f"ğŸ” ë“£ê¸° ë¬¸ì œ ê²€ì¦ (ì‹œë„ {attempt}):")
        print(f"   - [AUDIO]: {has_audio_tag}")
        print(f"   - [SPEAKERS]: {has_speakers_tag}")
        print(f"   - ëŒ€í™” í˜•ì‹ (Name:): {len(dialogue_matches)}ê°œ")
        print(f"   - ì²« ë°œí™” (í™”ì ì—†ìŒ): {len(first_utterance_matches)}ê°œ")

        # ëŒ€í™” í˜•ì‹ì´ ì—†ìœ¼ë©´ ë…ë°±í˜•ìœ¼ë¡œ ê°„ì£¼ (í™”ì ë¶„ë¦¬ ì—†ìŒ)
        if not has_dialogue:
            print(f"   â„¹ï¸  ë…ë°±í˜• ë“£ê¸° ë¬¸ì œ (í™”ì ë¶„ë¦¬ ì—†ìŒ)")
            print(f"   âœ… ë“£ê¸° ë¬¸ì œ ê²€ì¦ í†µê³¼ (ë…ë°±í˜•)!")
            return '\n'.join(lines)  # í›„ì²˜ë¦¬ ì—†ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜

        # [AUDIO]: íƒœê·¸ ê°•ì œ ì¶”ê°€
        if not has_audio_tag:
            print(f"   âš ï¸  [AUDIO]: íƒœê·¸ ì—†ìŒ â†’ ê°•ì œ ì¶”ê°€")
            # Find first dialogue line or first utterance
            first_dialogue_idx = None
            for i, line in enumerate(lines):
                if dialogue_pattern.match(line.strip()) or first_utterance_pattern.match(line.strip()):
                    first_dialogue_idx = i
                    break

            if first_dialogue_idx is not None:
                lines.insert(first_dialogue_idx, '[AUDIO]:')

        # [SPEAKERS]: JSON ìë™ ìƒì„± ë˜ëŠ” voice enhancement
        if (not has_speakers_tag or needs_voice_enhancement) and has_dialogue:
            if not has_speakers_tag:
                print(f"   âš ï¸  [SPEAKERS]: íƒœê·¸ ì—†ìŒ â†’ ìë™ ìƒì„±")
            else:
                print(f"   âš ï¸  [SPEAKERS]: voice í•„ë“œ ì¶”ê°€ ì¤‘...")

            # Extract unique speaker names from existing labels
            unique_speakers = []
            seen = set()
            for match in dialogue_matches:
                if match not in seen:
                    unique_speakers.append(match)
                    seen.add(match)

            # If only 1 speaker found but we need 2, add a default second speaker
            if len(unique_speakers) == 1:
                # Infer second speaker from context
                first_name = unique_speakers[0]
                # Default second names
                if first_name.lower() in ['emma', 'sarah', 'lisa', 'maria', 'anna']:
                    second_name = 'Jake'  # Male counterpart
                else:
                    second_name = 'Emma'  # Female counterpart
                unique_speakers.insert(0, second_name)  # Add as first speaker
                print(f"   â„¹ï¸  í™”ì 1ëª…ë§Œ ê°ì§€ â†’ ë‘ ë²ˆì§¸ í™”ì ì¶”ê°€: {second_name}")

            # Use LLM to determine gender and voice for each speaker
            # Build a prompt for the LLM
            speaker_names = ", ".join(unique_speakers)
            llm_prompt = f"""Given these speaker names: {speaker_names}

For EACH speaker, determine:
1. Gender (male or female)
2. US English voice name - IMPORTANT: Assign DIFFERENT voices to DIFFERENT speakers!

Available voices:
- Female voices: Samantha, Karen, Victoria
- Male voices: David, Daniel, Mark

CRITICAL RULES:
- If you have 2 speakers, they MUST use DIFFERENT voice names
- Never assign the same voice to multiple speakers in one conversation
- Example (CORRECT): Speaker1 uses Samantha, Speaker2 uses David
- Example (WRONG): Speaker1 uses Samantha, Speaker2 uses Samantha

Respond with ONLY valid JSON:
{{
  "speakers": [
    {{"name": "Name1", "gender": "female", "voice": "Samantha"}},
    {{"name": "Name2", "gender": "male", "voice": "David"}}
  ]
}}"""

            try:
                # Call LLM to determine speakers (o4-mini - ì¶”ë¡  ëª¨ë¸)
                llm_response = self.client.chat.completions.create(
                    model="o4-mini",  # Reasoning model for speaker analysis
                    messages=[{"role": "user", "content": llm_prompt}],
                    max_completion_tokens=300
                )

                # o4-mini doesn't support JSON mode, parse from text
                content_llm = llm_response.choices[0].message.content
                # Extract JSON from response (handle markdown code blocks)
                json_match = re.search(r'\{.*\}', content_llm, re.DOTALL)
                if json_match:
                    speakers_json = json.loads(json_match.group(0))
                else:
                    speakers_json = json.loads(content_llm)

                print(f"   âœ… o4-miniê°€ í™”ì ì •ë³´ ìƒì„±: {speakers_json}")

            except Exception as e:
                print(f"   âš ï¸  LLM í˜¸ì¶œ ì‹¤íŒ¨, fallback to rules: {e}")
                # Fallback to rule-based
                female_names = {'sarah', 'emma', 'lisa', 'maria', 'anna', 'kate', 'jane', 'mary', 'lucy', 'emily'}
                male_names = {'tom', 'john', 'mike', 'david', 'james', 'robert', 'mark', 'peter', 'kevin', 'alex', 'jake'}

                speakers_json = {"speakers": []}
                for name in unique_speakers:
                    name_lower = name.lower()
                    if name_lower in female_names:
                        gender = "female"
                        voice = "Samantha"
                    elif name_lower in male_names:
                        gender = "male"
                        voice = "David"
                    else:
                        # Default: alternate male/female
                        gender = "female" if len(speakers_json["speakers"]) % 2 == 0 else "male"
                        voice = "Samantha" if gender == "female" else "David"

                    speakers_json["speakers"].append({"name": name, "gender": gender, "voice": voice})

            # Insert or Replace [SPEAKERS]: line
            speakers_line = f"[SPEAKERS]: {json.dumps(speakers_json)}"

            if needs_voice_enhancement:
                # Replace existing [SPEAKERS]: line
                for i, line in enumerate(lines):
                    if '[SPEAKERS]:' in line:
                        lines[i] = speakers_line
                        print(f"   âœ… [SPEAKERS]: voice í•„ë“œ ì¶”ê°€ ì™„ë£Œ ({len(speakers_json['speakers'])}ëª…)")
                        break
            else:
                # Insert new [SPEAKERS]: line after [AUDIO]:
                audio_idx = None
                for i, line in enumerate(lines):
                    if '[AUDIO]:' in line:
                        audio_idx = i
                        break

                if audio_idx is not None:
                    lines.insert(audio_idx + 1, speakers_line)
                    print(f"   âœ… [SPEAKERS]: ìë™ ìƒì„± ì™„ë£Œ ({len(speakers_json['speakers'])}ëª…)")

        # 4. ì²« ë°œí™”ì— í™”ì ì´ë¦„ ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
        # Re-parse speakers JSON to get first speaker
        speakers_match = re.search(r'\[SPEAKERS\]:\s*({.*})', '\n'.join(lines))
        if speakers_match:
            try:
                speakers_data = json.loads(speakers_match.group(1))
                speakers_list = speakers_data.get('speakers', [])

                if speakers_list:
                    first_speaker = speakers_list[0]['name']

                    # Find first line after [SPEAKERS]: that looks like dialogue but has no speaker label
                    speakers_idx = None
                    for i, line in enumerate(lines):
                        if '[SPEAKERS]:' in line:
                            speakers_idx = i
                            break

                    if speakers_idx is not None:
                        # Check next few lines for dialogue without speaker
                        for i in range(speakers_idx + 1, min(speakers_idx + 5, len(lines))):
                            line = lines[i].strip()
                            # Skip empty lines and lines that already have speaker labels
                            if not line or dialogue_pattern.match(line):
                                continue
                            # If it's a sentence (starts with capital, has words)
                            if re.match(r'^[A-Z][^:]{10,}', line):
                                # Add first speaker name
                                lines[i] = f"{first_speaker}: {line}"
                                print(f"   âœ… ì²« ë°œí™”ì— í™”ì ì¶”ê°€: {first_speaker}:")
                                break
            except Exception as e:
                print(f"   âš ï¸  ì²« ë°œí™” í™”ì ì¶”ê°€ ì‹¤íŒ¨: {e}")

        result = '\n'.join(lines)

        # Final verification
        final_has_audio = '[AUDIO]:' in result
        final_has_speakers = '[SPEAKERS]:' in result
        final_dialogue_count = len(re.findall(r'^([A-Z][a-z]+):\s+', result, re.MULTILINE))

        print(f"   ğŸ“Š ìµœì¢… ëŒ€í™” ìˆ˜: {final_dialogue_count}ê°œ")

        if final_has_audio and final_has_speakers and final_dialogue_count >= 2:
            print(f"   âœ… ë“£ê¸° ë¬¸ì œ ê²€ì¦ í†µê³¼!")
        else:
            print(f"   âš ï¸  ìµœì¢… ê²€ì¦ - AUDIO: {final_has_audio}, SPEAKERS: {final_has_speakers}, ëŒ€í™”: {final_dialogue_count}ê°œ")

        # ğŸ™ï¸ OpenAI TTSë¡œ ê³ í’ˆì§ˆ ìŒì„± ìƒì„±
        if final_has_audio and final_has_speakers:
            try:
                print(f"   ğŸ™ï¸  OpenAI TTS ìŒì„± ìƒì„± ì¤‘...")
                tts_service = get_tts_service()
                audio_url = tts_service.get_or_create_audio(result, session_id=self.current_session_id)

                if audio_url:
                    # Add audio URL to the beginning of the problem (for frontend to use)
                    result = f"[AUDIO_URL]: {audio_url}\n\n{result}"
                    print(f"   âœ… TTS ìŒì„± ìƒì„± ì™„ë£Œ: {audio_url}")
                else:
                    print(f"   âš ï¸  TTS ìŒì„± ìƒì„± ì‹¤íŒ¨ - ë¸Œë¼ìš°ì € TTS ì‚¬ìš©")
            except Exception as e:
                print(f"   âš ï¸  TTS ì˜¤ë””ì˜¤ ìƒì„± ì˜¤ë¥˜: {e}")
                print(f"   â„¹ï¸  ë¸Œë¼ìš°ì € TTS fallback ì‚¬ìš©")

        return result

    def _add_parent_guidance(self, problem_content: str, difficulty: str, topic: str) -> str:
        """
        ë“£ê¸° ë¬¸ì œì— í•™ë¶€ëª¨ ì§€ë„ ê°€ì´ë“œ ì¶”ê°€

        Args:
            problem_content: ìƒì„±ëœ ë“£ê¸° ë¬¸ì œ
            difficulty: CEFR ë ˆë²¨
            topic: ì£¼ì œ

        Returns:
            ë¬¸ì œ + í•™ë¶€ëª¨ ì§€ë„ ê°€ì´ë“œ
        """
        try:
            print(f"   ğŸ“š í•™ë¶€ëª¨ ì§€ë„ ê°€ì´ë“œ ìƒì„± ì¤‘...")

            # ë¬¸ì œì—ì„œ ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ (ê°„ë‹¨í•˜ê²Œ [AUDIO]: ì´í›„ ë¶€ë¶„)
            import re
            audio_match = re.search(r'\[AUDIO\]:(.+?)(?:\n\n|\n(?=[A-Z])|\Z)', problem_content, re.DOTALL)
            dialogue_sample = audio_match.group(1).strip()[:300] if audio_match else ""

            guidance_prompt = f"""ë‹¹ì‹ ì€ ì˜ì–´ êµìœ¡ ì „ë¬¸ê°€ì´ì í•™ë¶€ëª¨ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë“£ê¸° ë¬¸ì œë¥¼ ìë…€ì™€ í•¨ê»˜ í•™ìŠµí•  ë•Œ, í•™ë¶€ëª¨ë‹˜ì´ ì–´ë–»ê²Œ ì§€ë„í•˜ë©´ ì¢‹ì„ì§€ **ì‹¤ì²œ ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ê°€ì´ë“œ**ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ë“£ê¸° ë¬¸ì œ ì •ë³´:**
- CEFR ë ˆë²¨: {difficulty}
- ì£¼ì œ: {topic}
- ëŒ€í™” ì˜ˆì‹œ: {dialogue_sample[:200]}...

**í•™ë¶€ëª¨ ì§€ë„ ê°€ì´ë“œ ì‘ì„± í˜•ì‹:**

---

## ğŸ“š í•™ë¶€ëª¨ë‹˜ì„ ìœ„í•œ ì§€ë„ ê°€ì´ë“œ

### 1ï¸âƒ£ ë“£ê¸° ì „ ì¤€ë¹„ (Pre-listening)
- ìë…€ì—ê²Œ ì£¼ì œ({topic})ì— ëŒ€í•´ ë¨¼ì € ì´ì•¼ê¸° ë‚˜ëˆ„ê¸°
  ì˜ˆ: "ì˜¤ëŠ˜ì€ {topic}ì— ê´€í•œ ëŒ€í™”ë¥¼ ë“¤ì„ ê±°ì•¼. ë„ˆëŠ” {topic}ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•´?"
- ì£¼ìš” ë‹¨ì–´ 2-3ê°œë¥¼ ë¯¸ë¦¬ ì„¤ëª… (í•„ìš”ì‹œ)

### 2ï¸âƒ£ ë“£ê¸° ì¤‘ í™œë™ (While-listening)
- ì²« ë²ˆì§¸ ë“£ê¸°: ì „ì²´ ë‚´ìš© íŒŒì•… (ì •ë‹µ ë§íˆê¸°ë³´ë‹¤ ì´í•´í•˜ê¸°)
- ë‘ ë²ˆì§¸ ë“£ê¸°: ë¬¸ì œ í’€ë©´ì„œ ë“£ê¸°
- **TIP**: ìë…€ê°€ ì–´ë ¤ì›Œí•˜ë©´ ìë§‰(ìŠ¤í¬ë¦½íŠ¸)ì„ í•¨ê»˜ ë³´ë©´ì„œ ë“¤ì–´ë„ OK!

### 3ï¸âƒ£ ë“£ê¸° í›„ í™œë™ (Post-listening)
- ì •ë‹µ í™•ì¸ ë° ì™œ ê·¸ëŸ°ì§€ í•¨ê»˜ ìƒê°í•´ë³´ê¸°
- ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì²œì²œíˆ ì½ì–´ë³´ë©° ëª¨ë¥´ëŠ” ë‹¨ì–´ ì°¾ê¸°
- ìë…€ì™€ ì—­í•  ë†€ì´ë¡œ ëŒ€í™” ë”°ë¼ ì½ê¸° (ë°œìŒ ì—°ìŠµ)

### 4ï¸âƒ£ ê°€ì •ì—ì„œ í•  ìˆ˜ ìˆëŠ” í™œë™ (5-10ë¶„)
- ë¹„ìŠ·í•œ ìƒí™© ì—­í• ê·¹ (ìë…€ì™€ í•¨ê»˜ ì—°ê¸°í•˜ê¸°)
- ëŒ€í™”ì˜ í•œ ë¶€ë¶„ì„ ìš°ë¦¬ë§ë¡œ ë°”ê¿” ë§í•˜ê¸°
- "{topic}" ê´€ë ¨ ì˜ì–´ ë‹¨ì–´ 3ê°œ ë” ì°¾ì•„ë³´ê¸°

### 5ï¸âƒ£ í•™ë¶€ëª¨ë‹˜ê»˜ ë‹¹ë¶€ ë§ì”€
- ì •ë‹µì„ ëª» ë§í˜€ë„ ê´œì°®ìŠµë‹ˆë‹¤. ì˜ì–´ë¥¼ ë“£ê³  ì´í•´í•˜ëŠ” ê³¼ì • ìì²´ê°€ ì¤‘ìš”í•´ìš”.
- ìë…€ê°€ ì˜ì–´ë¡œ ëŒ€ë‹µí•˜ì§€ ëª»í•´ë„ í•œê¸€ë¡œ ì„¤ëª…í•˜ë©´ ì´í•´í•œ ê²ƒì…ë‹ˆë‹¤.
- ë§¤ì¼ 5ë¶„ì”© ê¾¸ì¤€íˆ í•˜ëŠ” ê²ƒì´ í•˜ë£¨ 30ë¶„ ëª°ì•„ì„œ í•˜ëŠ” ê²ƒë³´ë‹¤ íš¨ê³¼ì ì…ë‹ˆë‹¤.

---

ìœ„ í˜•ì‹ìœ¼ë¡œ ì´ ë“£ê¸° ë¬¸ì œì— ë§ëŠ” **êµ¬ì²´ì ì´ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ** ê°€ì´ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , í•™ë¶€ëª¨ë‹˜ì´ ë°”ë¡œ ë”°ë¼ í•  ìˆ˜ ìˆë„ë¡ ì˜ˆì‹œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”."""

            # ëª¨ë¸ ì„ íƒ: ì‹¤ì²œ ê°€ëŠ¥í•œ ê°„ë‹¨í•œ ê°€ì´ë“œ â†’ gpt-4.1-mini
            model = "gpt-4.1-mini"
            print(f"ğŸ“Š í•™ë¶€ëª¨ ì§€ë„ ê°€ì´ë“œ ìƒì„± ëª¨ë¸: {model}")

            response = self.client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜ì–´ êµìœ¡ ì „ë¬¸ê°€ì´ì í•™ë¶€ëª¨ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": guidance_prompt}
                ],
                temperature=0.7,
                max_tokens=1200
            )

            guidance = response.choices[0].message.content

            # ë¬¸ì œ + ì§€ë„ ê°€ì´ë“œ ê²°í•©
            result = f"{problem_content}\n\n{guidance}"

            print(f"   âœ… í•™ë¶€ëª¨ ì§€ë„ ê°€ì´ë“œ ì¶”ê°€ ì™„ë£Œ")
            return result

        except Exception as e:
            print(f"   âš ï¸  ì§€ë„ ê°€ì´ë“œ ìƒì„± ì‹¤íŒ¨: {e}, ë¬¸ì œë§Œ ë°˜í™˜")
            return problem_content  # ì‹¤íŒ¨ ì‹œ ë¬¸ì œë§Œ ë°˜í™˜

    def _parse_quick_reply(self, content: str, used_models: List[str]) -> Dict[str, Any]:
        """
        ì‘ë‹µì—ì„œ [QUICK_REPLY:...] íŒ¨í„´ì„ íŒŒì‹±í•˜ì—¬ quick_replies í•„ë“œë¡œ ë³€í™˜

        Format: [QUICK_REPLY:VO|RD|WR|LS|GR]
        ê° ì½”ë“œëŠ” í´ë¦­ ê°€ëŠ¥í•œ ë²„íŠ¼ìœ¼ë¡œ ë³€í™˜ë¨
        """
        import re

        # Quick reply íŒ¨í„´ ê²€ìƒ‰
        pattern = r'\[QUICK_REPLY:(.*?)\]'
        match = re.search(pattern, content)

        if match:
            # Quick reply ì½”ë“œ ì¶”ì¶œ (ì˜ˆ: "VO|RD|WR|LS|GR")
            codes = match.group(1).split('|')

            # ì½”ë“œë¥¼ í•œê¸€ ì´ë¦„ê³¼ ë§¤í•‘
            code_mapping = {
                "VO": {"label": "ì–´íœ˜ (Vocabulary)", "value": "ì–´íœ˜ ë¬¸ì œ ë‚´ì¤˜"},
                "RD": {"label": "ë…í•´ (Reading)", "value": "ë…í•´ ë¬¸ì œ ë‚´ì¤˜"},
                "WR": {"label": "ì“°ê¸° (Writing)", "value": "ì“°ê¸° ë¬¸ì œ ë‚´ì¤˜"},
                "LS": {"label": "ë“£ê¸° (Listening)", "value": "ë“£ê¸° ë¬¸ì œ ë‚´ì¤˜"},
                "GR": {"label": "ë¬¸ë²• (Grammar)", "value": "ë¬¸ë²• ë¬¸ì œ ë‚´ì¤˜"}
            }

            # Quick replies ìƒì„±
            quick_replies = []
            for code in codes:
                code = code.strip()
                if code in code_mapping:
                    quick_replies.append(code_mapping[code])

            # íŒ¨í„´ ì œê±°í•œ ë©”ì‹œì§€
            clean_message = re.sub(pattern, '', content).strip()

            return {
                "message": clean_message,
                "quick_replies": quick_replies,
                "model_info": {
                    "primary": "o4-mini (ReAct)" if "ReAct" in str(used_models) else "gpt-4.1-mini",
                    "all_used": used_models
                }
            }
        else:
            # Quick reply ì—†ìœ¼ë©´ ì¼ë°˜ ì‘ë‹µ
            return {
                "message": content,
                "model_info": {
                    "primary": "o4-mini (ReAct)" if "ReAct" in str(used_models) else "gpt-4.1-mini",
                    "all_used": used_models
                }
            }

    def _lookup_word(self, word: str) -> str:
        """ì˜ì–´ ë‹¨ì–´ ê²€ìƒ‰ (Free Dictionary API)"""
        try:
            service = get_dictionary_service()
            result = service.lookup_word(word)

            if not result.get('success'):
                return f"ë‹¨ì–´ '{word}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì² ìë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

            # ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ í¬ë§·íŒ…
            response = f"**{result['word']}** {result.get('phonetic', '')}\n\n"
            response += f"**í’ˆì‚¬:** {result.get('part_of_speech', 'N/A')}\n\n"
            response += f"**ì •ì˜:** {result.get('definition', 'N/A')}\n\n"

            if result.get('example'):
                response += f"**ì˜ˆë¬¸:** {result['example']}\n\n"

            if result.get('synonyms'):
                synonyms = ', '.join(result['synonyms'])
                response += f"**ë™ì˜ì–´:** {synonyms}\n\n"

            if result.get('antonyms'):
                antonyms = ', '.join(result['antonyms'])
                response += f"**ë°˜ì˜ì–´:** {antonyms}\n\n"

            # ë°œìŒ ì˜¤ë””ì˜¤ URLì´ ìˆìœ¼ë©´ ì¶”ê°€
            phonetics = result.get('phonetics', [])
            audio_urls = [p['audio'] for p in phonetics if p.get('audio')]
            if audio_urls:
                response += f"**ë°œìŒ ë“£ê¸°:** {audio_urls[0]}\n"

            return response
        except Exception as e:
            return f"ë‹¨ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"

    def _fetch_news(self, category: str = "general", page_size: int = 3) -> str:
        """ì˜ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰ (NewsAPI)"""
        try:
            service = get_news_service()
            result = service.fetch_news(
                category=category,
                language="en",
                page_size=min(page_size, 5),  # ìµœëŒ€ 5ê°œ
                country="us"
            )

            if not result.get('success'):
                error_msg = result.get('error', 'Unknown error')
                if "NEWS_API_KEY not configured" in error_msg:
                    return "ë‰´ìŠ¤ APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
                return f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {error_msg}"

            articles = result.get('articles', [])
            if not articles:
                return f"'{category}' ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ê¸°ì‚¬ í¬ë§·íŒ…
            response = f"**{category.title()} ì¹´í…Œê³ ë¦¬ ìµœì‹  ë‰´ìŠ¤** ({len(articles)}ê°œ)\n\n"

            for i, article in enumerate(articles, 1):
                response += f"**{i}. {article['title']}**\n"
                response += f"   *ì¶œì²˜:* {article['source']}\n"
                if article.get('description'):
                    response += f"   *ìš”ì•½:* {article['description']}\n"
                response += f"   *ë§í¬:* {article['url']}\n\n"

            return response
        except Exception as e:
            return f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"

    def _analyze_text_difficulty(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„ (textstat)"""
        try:
            service = get_text_analysis_service()
            result = service.analyze_cefr_level(text)

            if not result.get('success'):
                error_msg = result.get('error', 'Unknown error')
                if "textstat library not installed" in error_msg:
                    return "í…ìŠ¤íŠ¸ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
                return f"í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {error_msg}"

            # ê²°ê³¼ í¬ë§·íŒ…
            response = f"**í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„ ê²°ê³¼**\n\n"
            response += f"**CEFR ë ˆë²¨:** {result['cefr_level']}\n"
            response += f"**ë‚œì´ë„:** {result['difficulty']}\n"
            response += f"**ê°€ë…ì„± ì ìˆ˜ (Flesch):** {result['flesch_reading_ease']}/100 (ë†’ì„ìˆ˜ë¡ ì‰¬ì›€)\n"
            response += f"**í•™ë…„ ìˆ˜ì¤€ (FK Grade):** {result['flesch_kincaid_grade']}í•™ë…„\n\n"
            response += f"**í†µê³„:**\n"
            response += f"- ë‹¨ì–´ ìˆ˜: {result['word_count']}ê°œ\n"
            response += f"- ë¬¸ì¥ ìˆ˜: {result['sentence_count']}ê°œ\n"
            response += f"- í‰ê·  ë¬¸ì¥ ê¸¸ì´: {result['avg_sentence_length']}ë‹¨ì–´\n\n"

            # ë ˆë²¨ë³„ ì„¤ëª…
            level_descriptions = {
                "A1": "ì´ˆê¸‰ - ë§¤ìš° ì‰¬ìš´ í…ìŠ¤íŠ¸",
                "A2": "ì´ˆê¸‰ ìƒ - ì‰¬ìš´ í…ìŠ¤íŠ¸",
                "B1": "ì¤‘ê¸‰ - ë³´í†µ ë‚œì´ë„ í…ìŠ¤íŠ¸",
                "B2": "ì¤‘ê¸‰ ìƒ - ì–´ë ¤ìš´ í…ìŠ¤íŠ¸",
                "C1": "ê³ ê¸‰ - ë§¤ìš° ì–´ë ¤ìš´ í…ìŠ¤íŠ¸",
                "C2": "ê³ ê¸‰ ìƒ - ì „ë¬¸ê°€ ìˆ˜ì¤€ í…ìŠ¤íŠ¸"
            }
            response += f"**ë ˆë²¨ ì„¤ëª…:** {level_descriptions.get(result['cefr_level'], 'N/A')}\n"

            return response
        except Exception as e:
            return f"í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    def _check_grammar(self, text: str) -> str:
        """ë¬¸ë²• ê²€ì‚¬ (LanguageTool API)"""
        try:
            service = get_grammar_check_service()
            result = service.check_grammar(text, language="en-US")

            if not result.get('success'):
                return f"ë¬¸ë²• ê²€ì‚¬ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}"

            error_count = result.get('error_count', 0)
            errors = result.get('errors', [])

            if error_count == 0:
                return "**ë¬¸ë²• ê²€ì‚¬ ê²°ê³¼:** ë¬¸ë²• ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤! ì˜í–ˆì–´ìš”! âœ…"

            # ì˜¤ë¥˜ í¬ë§·íŒ…
            response = f"**ë¬¸ë²• ê²€ì‚¬ ê²°ê³¼:** {error_count}ê°œì˜ ì˜¤ë¥˜ ë°œê²¬\n\n"

            for i, error in enumerate(errors[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                response += f"**{i}. {error['message']}**\n"

                # ì˜¤ë¥˜ ìœ„ì¹˜ í‘œì‹œ
                offset = error['offset']
                length = error['length']
                error_text = text[offset:offset+length]
                response += f"   *ë¬¸ì œ:* \"{error_text}\"\n"

                # ìˆ˜ì • ì œì•ˆ
                replacements = error.get('replacements', [])
                if replacements:
                    suggestions = ', '.join([f'"{r}"' for r in replacements])
                    response += f"   *ì œì•ˆ:* {suggestions}\n"

                # ì¹´í…Œê³ ë¦¬
                category = error.get('category', '')
                if category:
                    response += f"   *ìœ í˜•:* {category}\n"

                response += "\n"

            if error_count > 10:
                response += f"*({error_count - 10}ê°œ ì˜¤ë¥˜ ë” ìˆìŒ)*\n"

            return response
        except Exception as e:
            return f"ë¬¸ë²• ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}"

    def _search_youtube(
        self,
        student_id: str,
        topic: str,
        search_type: str = "video",
        max_results: int = 5
    ) -> str:
        """YouTube ì˜ìƒ/ì±„ë„ ê²€ìƒ‰ (ìë…€ í•™ìŠµìš©)"""
        try:
            # í•™ìƒ CEFR ë ˆë²¨ ì¡°íšŒ
            student_info = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text="CEFR ë ˆë²¨",
                use_vector_search=False
            )

            cefr_level = "A1"  # ê¸°ë³¸ê°’
            if student_info:
                import re
                cefr_match = re.search(r'CEFR ë ˆë²¨: ([A-C][1-2])', student_info)
                if cefr_match:
                    cefr_level = cefr_match.group(1)

            # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” (CEFR ë ˆë²¨ + ì£¼ì œ)
            optimized_query = f"English {topic} {cefr_level} level for kids education"

            print(f"ğŸ¥ YouTube Search: {optimized_query} (type: {search_type})")

            youtube_service = get_youtube_service()

            if search_type == "channel":
                results = youtube_service.search_channels(
                    query=optimized_query,
                    max_results=max_results
                )
            else:
                results = youtube_service.search_videos(
                    query=optimized_query,
                    max_results=max_results,
                    video_duration="medium"  # 4-20ë¶„ ì˜ìƒ
                )

            if not results:
                return f"ì£„ì†¡í•©ë‹ˆë‹¤. '{topic}' ê´€ë ¨ YouTube {search_type}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì£¼ì œë¡œ ì‹œë„í•´ë³´ì„¸ìš”."

            # ê²°ê³¼ í¬ë§·íŒ…
            if search_type == "channel":
                response = f"**ğŸ¬ '{topic}' ê´€ë ¨ YouTube ì±„ë„ ì¶”ì²œ (CEFR {cefr_level} ë ˆë²¨):**\n\n"
                for i, channel in enumerate(results, 1):
                    response += f"**{i}. {channel['title']}**\n"
                    response += f"   ğŸ“º ì±„ë„: {channel['url']}\n"
                    if channel['description']:
                        desc = channel['description'][:100] + "..." if len(channel['description']) > 100 else channel['description']
                        response += f"   ğŸ“ {desc}\n"
                    response += "\n"
            else:
                response = f"**ğŸ¬ '{topic}' ê´€ë ¨ YouTube ì˜ìƒ ì¶”ì²œ (CEFR {cefr_level} ë ˆë²¨):**\n\n"
                for i, video in enumerate(results, 1):
                    response += f"**{i}. {video['title']}**\n"
                    response += f"   ğŸ“º ì±„ë„: {video['channel']}\n"
                    response += f"   ğŸ”— ë§í¬: {video['url']}\n"
                    if video['description']:
                        desc = video['description'][:100] + "..." if len(video['description']) > 100 else video['description']
                        response += f"   ğŸ“ {desc}\n"
                    response += "\n"

            response += "\nğŸ’¡ **ê°€ì • í•™ìŠµ íŒ:**\n"
            response += "â€¢ ìë…€ì™€ í•¨ê»˜ ì˜ìƒì„ ì‹œì²­í•˜ì„¸ìš”\n"
            response += "â€¢ ì˜ìƒì„ ë³¸ í›„ ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ì´í•´ë„ë¥¼ í™•ì¸í•˜ì„¸ìš”\n"
            response += "â€¢ í•˜ë£¨ 10-15ë¶„ ì •ë„ê°€ ì ë‹¹í•©ë‹ˆë‹¤\n"

            return response

        except Exception as e:
            return f"YouTube ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}\n\nì§ì ‘ YouTubeì—ì„œ '{topic}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."

    def _execute_function(self, function_name: str, arguments: Dict[str, Any]) -> str:
        """Function ì‹¤í–‰"""

        # í•¨ìˆ˜ íƒ€ì… ë¶„ë¥˜
        db_functions = ["get_child_info", "analyze_performance", "get_attendance_status"]
        generation_functions = ["get_study_advice", "recommend_improvement_areas", "generate_problem"]
        external_api_functions = ["lookup_word", "fetch_news", "analyze_text_difficulty", "check_grammar", "search_youtube"]

        if function_name in db_functions:
            func_type = "ğŸ“Š DATABASE QUERY"
            color = Colors.GREEN
        elif function_name in generation_functions:
            func_type = "ğŸ¤– AI GENERATION"
            color = Colors.MAGENTA
        elif function_name in external_api_functions:
            func_type = "ğŸŒ EXTERNAL API"
            color = Colors.BLUE
        else:
            func_type = "â“ UNKNOWN"
            color = Colors.RED

        print_section(
            "ğŸ”§",
            f"FUNCTION CALL - {func_type}",
            f"Function: {function_name}\n"
            f"Arguments: {json.dumps(arguments, ensure_ascii=False, indent=2)}",
            color
        )

        if function_name == "get_child_info":
            # ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•´ í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì „ë‹¬
            result = self._get_child_info(query_text=self.current_user_message, **arguments)
        elif function_name == "analyze_performance":
            result = self._analyze_performance(**arguments)
        elif function_name == "get_study_advice":
            result = self._get_study_advice(**arguments)
        elif function_name == "get_attendance_status":
            result = self._get_attendance_status(**arguments)
        elif function_name == "recommend_improvement_areas":
            result = self._recommend_improvement_areas(**arguments)
        elif function_name == "generate_problem":
            result = self._generate_problem(**arguments)
        elif function_name == "lookup_word":
            result = self._lookup_word(**arguments)
        elif function_name == "fetch_news":
            result = self._fetch_news(**arguments)
        elif function_name == "analyze_text_difficulty":
            result = self._analyze_text_difficulty(**arguments)
        elif function_name == "check_grammar":
            result = self._check_grammar(**arguments)
        elif function_name == "search_youtube":
            result = self._search_youtube(**arguments)
        else:
            result = f"Unknown function: {function_name}"

        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥
        result_preview = result[:200] + "..." if len(result) > 200 else result
        print(f"{Colors.GREEN}âœ… Function completed: {result_preview}{Colors.RESET}\n")

        return result

    def chat(
        self,
        student_id: str,
        message: str,
        chat_history: Optional[List[Dict[str, str]]] = None,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        í•™ë¶€ëª¨ì™€ ì±„íŒ… (Function Calling)

        Args:
            student_id: ìë…€ì˜ í•™ìƒ ID
            message: í•™ë¶€ëª¨ì˜ ë©”ì‹œì§€
            chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒ)
            session_id: ì„¸ì…˜ ID (ì˜¤ë””ì˜¤ ì¶”ì ìš©, ì„ íƒ)

        Returns:
            Dict with 'message' and 'model_info'
        """
        try:
            # ========== ìš”ì²­ ì‹œì‘ ë¡œê¹… ==========
            print_box(
                "ğŸ‘ª PARENT CHATBOT REQUEST",
                f"Child ID: {student_id}\n"
                f"Message: {message}\n"
                f"Session ID: {session_id or 'None'}",
                Colors.MAGENTA
            )

            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ (ë²¡í„° ê²€ìƒ‰ìš©)
            self.current_user_message = message

            # í˜„ì¬ ì„¸ì…˜ ID ì €ì¥ (ì˜¤ë””ì˜¤ ì¶”ì ìš©)
            self.current_session_id = session_id

            # ReAct ëª¨ë“œ íŒë‹¨ (ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì§ˆë¬¸)
            if self._needs_react(message):
                return self._react_chat(student_id, message, chat_history)

            # "ë¬¸ì œ ë‚´ì¤˜" íŒ¨í„´ ê°ì§€ (ìœ í˜• ë¯¸ì§€ì •)
            import re
            if re.search(r'(?:ë¬¸ì œ|problem)\s*(?:ë‚´|ì¤˜|í’€|í•´)', message, re.IGNORECASE) and \
               not re.search(r'(?:ë“£ê¸°|ë…í•´|ë¬¸ë²•|ì–´íœ˜|ì“°ê¸°|listening|reading|grammar|vocabulary|writing|VO|RD|GR|LS|WR)', message, re.IGNORECASE):
                # ìœ í˜•ì´ ëª…ì‹œë˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ "ë¬¸ì œ ë‚´ì¤˜" ìš”ì²­
                print("ğŸ” ë¬¸ì œ ìœ í˜• ì„ íƒ ìš”ì²­ ê°ì§€ (í•™ë¶€ëª¨)")
                return {
                    "message": "ìë…€ë¥¼ ìœ„í•´ ì–´ë–¤ ìœ í˜•ì˜ ë¬¸ì œë¥¼ ì¤€ë¹„í•´ë“œë¦´ê¹Œìš”?",
                    "quick_replies": [
                        {"label": "ğŸ’¬ ë“£ê¸° (Listening)", "value": "ë“£ê¸° ë¬¸ì œ ë‚´ì¤˜"},
                        {"label": "ğŸ“– ë…í•´ (Reading)", "value": "ë…í•´ ë¬¸ì œ ë‚´ì¤˜"},
                        {"label": "âœï¸ ì“°ê¸° (Writing)", "value": "ì“°ê¸° ë¬¸ì œ ë‚´ì¤˜"},
                        {"label": "ğŸ“ ë¬¸ë²• (Grammar)", "value": "ë¬¸ë²• ë¬¸ì œ ë‚´ì¤˜"},
                        {"label": "ğŸ“š ì–´íœ˜ (Vocabulary)", "value": "ì–´íœ˜ ë¬¸ì œ ë‚´ì¤˜"}
                    ],
                    "model_info": {
                        "primary": "gpt-4.1-mini",
                        "all_used": ["gpt-4.1-mini"]
                    }
                }

            used_models = []  # Track models used

            # Step 1: Route the query to determine complexity
            routing_decision = self._route_query(message, student_id)
            used_models.append("gpt-4o-mini")  # Router model

            # Step 2: Select primary model based on routing decision
            if routing_decision == "reasoning":
                primary_model = "o4-mini"
                print(f"ğŸ¯ Using reasoning model: {primary_model}")
            else:
                primary_model = "gpt-4.1-mini"
                print(f"ğŸ¯ Using intelligence model: {primary_model}")

            # PromptManagerë¥¼ ì‚¬ìš©í•´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            system_prompt = PromptManager.get_system_prompt(
                role="parent",
                model=primary_model,
                context={"student_id": student_id}
            )

            # ë©”ì‹œì§€ êµ¬ì„± (ë©€í‹°í„´ ì§€ì›)
            messages = [{"role": "system", "content": system_prompt}]

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
            if chat_history:
                print(f"ğŸ’¬ Multi-turn enabled: {len(chat_history)} previous messages")
                messages.extend(chat_history)
            else:
                print(f"ğŸ’¬ Single-turn conversation (no history)")

            # ìµœì‹  ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            messages.append({"role": "user", "content": message})
            print(f"ğŸ“ Total messages sent to LLM: {len(messages)}")

            # Step 3: Call primary model with function calling
            used_models.append(primary_model)

            # Use appropriate parameters based on model type
            if primary_model == "o4-mini":
                # o4-mini uses max_completion_tokens
                response = self.client.chat.completions.create(
                    model="o4-mini",
                    messages=messages,
                    tools=self.functions,
                    tool_choice="auto",
                    max_completion_tokens=10000
                )
            else:
                # gpt-4.1-mini uses max_tokens
                response = self.client.chat.completions.create(
                    model="gpt-4.1-mini",
                    messages=messages,
                    tools=self.functions,
                    tool_choice="auto",
                    temperature=0.7
                )

            assistant_message = response.choices[0].message

            # Function í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
            if assistant_message.tool_calls:
                # Function ì‹¤í–‰
                messages.append(assistant_message)

                for tool_call in assistant_message.tool_calls:
                    function_name = tool_call.function.name
                    arguments = json.loads(tool_call.function.arguments)

                    print(f"ğŸ”§ Function Call: {function_name}({arguments})")

                    # Track special models used in functions
                    if function_name in ["get_study_advice", "recommend_improvement_areas"]:
                        used_models.append("gpt-4o")
                    elif function_name == "generate_problem":
                        used_models.append("o4-mini")

                    # Function ì‹¤í–‰
                    function_response = self._execute_function(function_name, arguments)

                    # ë“£ê¸° ë¬¸ì œì˜ ê²½ìš°: [AUDIO]ì™€ [SPEAKERS] íƒœê·¸ë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•´ ì§ì ‘ ë°˜í™˜
                    if function_name == "generate_problem" and arguments.get("area", "").lower() in ['ë“£ê¸°', 'listening', 'ls']:
                        # Check if response contains [AUDIO] or [SPEAKERS]
                        if '[AUDIO]' in function_response or '[SPEAKERS]' in function_response:
                            print("ğŸ§ ë“£ê¸° ë¬¸ì œ: [AUDIO]/[SPEAKERS] íƒœê·¸ ë³´ì¡´ì„ ìœ„í•´ ì§ì ‘ ë°˜í™˜")
                            return {
                                "message": function_response,
                                "model_info": {
                                    "primary": "o4-mini",
                                    "all_used": list(set(used_models))
                                }
                            }

                    # Function ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": function_response
                    })

                # Function ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
                # Use the same model as primary for consistency
                if primary_model == "o4-mini":
                    final_response = self.client.chat.completions.create(
                        model="o4-mini",
                        messages=messages,
                        max_completion_tokens=10000
                    )
                else:
                    final_response = self.client.chat.completions.create(
                        model="gpt-4.1-mini",
                        messages=messages,
                        temperature=0.7
                    )

                response_content = final_response.choices[0].message.content

                # Step 4: Quality check for o4-mini responses
                if primary_model == "o4-mini" and not self._check_response_quality(response_content):
                    print(f"âš ï¸  o4-mini response quality low, falling back to o3...")
                    used_models.append("o3")

                    # Retry with o3 (advanced reasoning)
                    try:
                        final_response_o3 = self.client.chat.completions.create(
                            model="o3",
                            messages=messages,
                            max_completion_tokens=10000
                        )
                        response_content = final_response_o3.choices[0].message.content
                        primary_model = "o3"  # Update primary model
                        print(f"âœ… o3 response generated successfully")
                    except Exception as e:
                        print(f"âš ï¸  o3 fallback failed: {e}, using o4-mini response anyway")

                return {
                    "message": response_content,
                    "model_info": {
                        "primary": primary_model,
                        "all_used": list(set(used_models))
                    }
                }
            else:
                # Function í˜¸ì¶œ ì—†ì´ ì§ì ‘ ì‘ë‹µ
                response_content = assistant_message.content

                # Step 4: Quality check for o4-mini responses (no function calls)
                if primary_model == "o4-mini" and not self._check_response_quality(response_content):
                    print(f"âš ï¸  o4-mini response quality low, falling back to o3...")
                    used_models.append("o3")

                    # Retry with o3 (advanced reasoning)
                    try:
                        final_response_o3 = self.client.chat.completions.create(
                            model="o3",
                            messages=messages,
                            max_completion_tokens=10000
                        )
                        response_content = final_response_o3.choices[0].message.content
                        primary_model = "o3"  # Update primary model
                        print(f"âœ… o3 response generated successfully")
                    except Exception as e:
                        print(f"âš ï¸  o3 fallback failed: {e}, using o4-mini response anyway")

                return {
                    "message": response_content,
                    "model_info": {
                        "primary": primary_model,
                        "all_used": list(set(used_models))
                    }
                }

        except Exception as e:
            return {
                "message": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "model_info": {"primary": "error", "all_used": []}
            }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_parent_agent_service = None


def get_parent_agent_service() -> ParentAgentService:
    """Parent Agent ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _parent_agent_service
    if _parent_agent_service is None:
        _parent_agent_service = ParentAgentService()
    return _parent_agent_service
