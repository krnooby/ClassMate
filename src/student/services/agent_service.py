# -*- coding: utf-8 -*-
"""
Student Agent Service
OpenAI Function Calling ê¸°ë°˜ í•™ìƒ ìƒë‹´ ì‹œìŠ¤í…œ
"""
from __future__ import annotations
import os
import json
from typing import List, Dict, Any, Optional
from openai import OpenAI
from shared.services import (
    get_graph_rag_service,
    get_dictionary_service,
    get_news_service,
    get_text_analysis_service,
    get_grammar_check_service
)
from shared.prompts import PromptManager
from shared.services.tts_service import get_tts_service


# ANSI ìƒ‰ìƒ ì½”ë“œ
class Colors:
    RESET = '\033[0m'
    BOLD = '\033[1m'

    # ìƒ‰ìƒ
    BLACK = '\033[30m'
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    WHITE = '\033[37m'

    # ë°°ê²½ìƒ‰
    BG_BLACK = '\033[40m'
    BG_RED = '\033[41m'
    BG_GREEN = '\033[42m'
    BG_YELLOW = '\033[43m'
    BG_BLUE = '\033[44m'
    BG_MAGENTA = '\033[45m'
    BG_CYAN = '\033[46m'
    BG_WHITE = '\033[47m'


def print_box(title: str, content: str, color: str = Colors.CYAN):
    """ë°•ìŠ¤ í˜•íƒœë¡œ ë¡œê·¸ ì¶œë ¥"""
    width = 80
    print(f"\n{color}{'='*width}{Colors.RESET}")
    print(f"{color}{Colors.BOLD}{title.center(width)}{Colors.RESET}")
    print(f"{color}{'='*width}{Colors.RESET}")
    for line in content.split('\n'):
        if line.strip():
            print(f"{color}  {line}{Colors.RESET}")
    print(f"{color}{'='*width}{Colors.RESET}\n")


def print_section(emoji: str, title: str, content: str, color: str = Colors.BLUE):
    """ì„¹ì…˜ í˜•íƒœë¡œ ë¡œê·¸ ì¶œë ¥"""
    print(f"\n{color}{Colors.BOLD}{emoji} {title}{Colors.RESET}")
    print(f"{color}{'-'*60}{Colors.RESET}")
    for line in content.split('\n'):
        if line.strip():
            print(f"{color}  {line}{Colors.RESET}")
    print()


class StudentAgentService:
    """í•™ìƒ ë§ì¶¤ Function Calling ì—ì´ì „íŠ¸"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=self.openai_api_key)
        self.graph_rag_service = get_graph_rag_service()

        # Current user message for vector search context
        self.current_user_message = ""

        # Current session ID for audio tracking
        self.current_session_id = None

        # Function definitions
        self.functions = self._create_functions()

    def _route_query(self, message: str, student_id: str) -> str:
        """
        ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ
        Returns: "intelligence" (gpt-4.1-mini) or "reasoning" (o4-mini/o3)
        """
        routing_prompt = f'''Analyze this student's question and choose the appropriate model:

**intelligence** (gpt-4.1-mini) - Fast, cost-effective for:
- Simple problem requests (ë¬¸ì œ ë‚´ì¤˜, ë“£ê¸° ë¬¸ì œ, ë…í•´ ë¬¸ì œ)
- Greetings and casual chat (ì•ˆë…•?, ì˜ ì§€ë‚´?, ê³ ë§ˆì›Œ)
- Basic function calls (ì ìˆ˜ ë³´ê¸°, íŒíŠ¸ ë‹¬ë¼)
- Quick answers (ì •ë‹µì´ ë­ì•¼?, ëª‡ ì ì´ì•¼?)
- Encouragement and simple feedback
Examples: "ë¬¸ì œ ë‚´ì¤˜", "ë“£ê¸° ë¬¸ì œ í’€ê²Œ", "ì•ˆë…•?", "íŒíŠ¸ ì¤˜", "ì •ë‹µ ì•Œë ¤ì¤˜", "ê³ ë§ˆì›Œ"

**reasoning** (o4-mini) - Deep thinking for:
- In-depth explanations (ë¬¸ë²• ê°œë… ì„¤ëª…, ì™œ ê·¸ëŸ°ì§€)
- Complex grammar concepts (ê°€ì •ë²•, ê´€ê³„ëŒ€ëª…ì‚¬ ì‹¬í™”)
- Multi-step problem solving (ì—¬ëŸ¬ ë‹¨ê³„ í’€ì´)
- Learning strategy advice (ì–´ë–»ê²Œ ê³µë¶€í•´ì•¼ í• ê¹Œ?)
- Analysis of mistakes (ì™œ í‹€ë ¸ëŠ”ì§€ ë¶„ì„)
Examples: "ì™œ ì´ ë‹µì´ í‹€ë ¸ì–´?", "ê°€ì •ë²• ê³¼ê±°ì™„ë£Œë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì¤˜", "ë…í•´ ì‹¤ë ¥ì„ ëŠ˜ë¦¬ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ ë¼?", "ì´ ë¬¸ì œ í’€ì´ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì¤˜", "beë™ì‚¬ì™€ ì¼ë°˜ë™ì‚¬ì˜ ì°¨ì´ë¥¼ ê¹Šê²Œ ì•Œë ¤ì¤˜"

Question: "{message}"

Respond with ONLY "intelligence" or "reasoning".'''

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # Cheap, fast router
                messages=[{"role": "user", "content": routing_prompt}],
                max_tokens=10,
                temperature=0
            )
            decision = response.choices[0].message.content.strip().lower()

            # ì‹œê°ì  ë¡œê¹…
            if decision == "intelligence":
                print_section(
                    "ğŸ§ ",
                    "ROUTING DECISION",
                    f"Query: {message[:50]}...\n"
                    f"Model: INTELLIGENCE (gpt-4.1-mini)\n"
                    f"Reason: Fast function calling for simple tasks",
                    Colors.CYAN
                )
            else:
                print_section(
                    "ğŸ§ ",
                    "ROUTING DECISION",
                    f"Query: {message[:50]}...\n"
                    f"Model: REASONING (o4-mini)\n"
                    f"Reason: Deep thinking required for complex explanation",
                    Colors.MAGENTA
                )

            return decision if decision in ["intelligence", "reasoning"] else "intelligence"
        except Exception as e:
            print(f"{Colors.RED}âš ï¸  Routing failed: {e}, defaulting to intelligence{Colors.RESET}")
            return "intelligence"  # Default fallback

    def _check_response_quality(self, response_text: str) -> bool:
        """
        Check if o4-mini response quality is acceptable
        Returns: True if good, False if needs o3 fallback
        """
        # Bad indicators: too short, generic, error patterns
        if len(response_text) < 50:
            return False
        if "ì£„ì†¡í•©ë‹ˆë‹¤" in response_text and "ì˜¤ë¥˜" in response_text:
            return False
        if response_text.count("...") > 3:  # Too many ellipses = incomplete
            return False
        return True

    def _needs_react(self, message: str) -> bool:
        """ReAct ëª¨ë“œê°€ í•„ìš”í•œ ë³µì¡í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨"""
        import re

        reasons = []

        # íŒ¨í„´ 1: ì—°ê²°ì–´ ("í•˜ê³ ", "ì°¾ì•„ì„œ")
        multi_task_keywords = ['í•˜ê³ ', 'ê·¸ë¦¬ê³ ', 'ì°¾ì•„ì„œ', 'í™•ì¸í•˜ê³ ', 'ì¡°íšŒí•˜ê³ ', 'ì•Œë ¤ì£¼ê³ ']
        for keyword in multi_task_keywords:
            if keyword in message and ('í•´ì¤˜' in message or 'ì£¼ì„¸ìš”' in message or 'ì¤˜' in message):
                reasons.append(f"Multi-task keyword detected: '{keyword}'")

        # íŒ¨í„´ 2: "ë¨¼ì €...ê·¸ë‹¤ìŒ"
        if 'ë¨¼ì €' in message and ('ê·¸ë‹¤ìŒ' in message or 'ê·¸ë¦¬ê³ ' in message):
            reasons.append("Sequential task pattern: 'ë¨¼ì €...ê·¸ë‹¤ìŒ'")

        # íŒ¨í„´ 3: ë™ì‚¬ 3ê°œ ì´ìƒ
        action_verbs = ['ì°¾', 'ë¶„ì„', 'ì¶”ì²œ', 'í™•ì¸', 'ì¡°íšŒ', 'ê²€ìƒ‰', 'ë¹„êµ', 'ìƒì„±', 'ì„¤ëª…', 'ì•Œë ¤']
        verb_count = sum(1 for verb in action_verbs if verb in message)
        if verb_count >= 3:
            reasons.append(f"Multiple action verbs: {verb_count} detected")

        needs_react = len(reasons) > 0

        if needs_react:
            print_section(
                "ğŸ”„",
                "ReAct MODE ACTIVATED",
                f"Query: {message}\n" + "\n".join(f"- {r}" for r in reasons),
                Colors.YELLOW
            )

        return needs_react

    def _react_chat(
        self,
        student_id: str,
        message: str,
        chat_history: Optional[List[Dict[str, str]]] = None,
        max_steps: int = 5
    ) -> Dict[str, Any]:
        """ReAct (Reasoning + Acting) ëª¨ë“œë¡œ ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì‘ì—… ì²˜ë¦¬"""

        print(f"\n{'='*60}")
        print(f"ğŸ”„ ReAct Mode Activated")
        print(f"Query: {message}")
        print(f"{'='*60}\n")

        # System prompt
        system_prompt = PromptManager.get_system_prompt(
            role="student_agent",
            model="o4-mini",
            context={"student_id": student_id}
        )

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        if chat_history:
            messages.extend(chat_history)

        # ì´ˆê¸° user ë©”ì‹œì§€
        messages.append({"role": "user", "content": message})

        used_models = ["o4-mini (ReAct)"]

        for step in range(1, max_steps + 1):
            print(f"\n--- ReAct Step {step}/{max_steps} ---")

            # LLM í˜¸ì¶œ (o4-mini)
            response = self.client.chat.completions.create(
                model="o4-mini",
                messages=messages,
                tools=self.functions,
                tool_choice="auto",
                max_completion_tokens=10000
            )

            assistant_message = response.choices[0].message

            # Thought ì¶œë ¥
            if assistant_message.content:
                print(f"ğŸ’­ Thought: {assistant_message.content[:200]}...")

            # Function í˜¸ì¶œì´ ìˆìœ¼ë©´ ì‹¤í–‰
            if assistant_message.tool_calls:
                messages.append(assistant_message)

                for tool_call in assistant_message.tool_calls:
                    function_name = tool_call.function.name
                    arguments = json.loads(tool_call.function.arguments)

                    print(f"ğŸ”§ Action: {function_name}({arguments})")

                    # Function ì‹¤í–‰
                    result = self._execute_function(function_name, arguments)

                    print(f"ğŸ“Š Observation: {result[:200]}...")

                    # Tool result ì¶”ê°€
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result
                    })
            else:
                # Final answer
                print(f"âœ… Final Answer Reached")

                content = assistant_message.content or "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                # Quick reply íŒŒì‹±
                parsed = self._parse_quick_reply(content, used_models)

                return parsed

        # ìµœëŒ€ ë‹¨ê³„ ë„ë‹¬ - ë§ˆì§€ë§‰ ì‘ë‹µ ìš”ì²­
        print(f"âš ï¸  Max steps reached, generating final answer...")

        messages.append({
            "role": "user",
            "content": "Based on the information you've gathered, provide your final answer."
        })

        final_response = self.client.chat.completions.create(
            model="o4-mini",
            messages=messages,
            max_completion_tokens=10000
        )

        content = final_response.choices[0].message.content or "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        parsed = self._parse_quick_reply(content, used_models)

        return parsed

    def _create_functions(self) -> List[Dict[str, Any]]:
        """OpenAI Function Callingìš© function ì •ì˜"""
        return [
            {
                "type": "function",
                "function": {
                    "name": "get_student_context",
                    "description": "í•™ìƒì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì´ë¦„, í•™ë…„, CEFR ë ˆë²¨, ê°•ì , ì•½ì , ì˜ì—­ë³„ ì ìˆ˜, ì¶œì„ë¥ , ìˆ™ì œ ì™„ë£Œìœ¨, ë°˜ ì •ë³´, ì‹œê°„í‘œ ë“±)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "í•™ìƒ ID (ì˜ˆ: S-01)"
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "recommend_problems",
                    "description": "DBì—ì„œ í•™ìƒì˜ ì•½ì ì— ë§ëŠ” ì˜ì–´ ë¬¸ì œë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "í•™ìƒ ID"
                            },
                            "area": {
                                "type": "string",
                                "description": "ë¬¸ì œ ì˜ì—­ (ë…í•´, ë¬¸ë²•, ì–´íœ˜, ë“£ê¸°, ì“°ê¸° ì¤‘ í•˜ë‚˜, ë˜ëŠ” nullì¼ ê²½ìš° ì•½ì  ìë™ ê°ì§€)",
                                "enum": ["ë…í•´", "ë¬¸ë²•", "ì–´íœ˜", "ë“£ê¸°", "ì“°ê¸°", None]
                            },
                            "limit": {
                                "type": "integer",
                                "description": "ë¬¸ì œ ê°œìˆ˜ (ê¸°ë³¸ê°’ 3)",
                                "default": 3
                            }
                        },
                        "required": ["student_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "generate_problem",
                    "description": "AIê°€ ìƒˆë¡œìš´ ì˜ì–´ ë¬¸ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ê³ í’ˆì§ˆ). ë“£ê¸° ë¬¸ì œëŠ” ìë™ìœ¼ë¡œ ëŒ€í™” ìŒì„±ê³¼ [AUDIO], [SPEAKERS] íƒœê·¸ê°€ í¬í•¨ë©ë‹ˆë‹¤. difficultyë¥¼ ìƒëµí•˜ë©´ í•™ìƒì˜ CEFR ë ˆë²¨ì— ë§ì¶° ìë™ ìƒì„±ë©ë‹ˆë‹¤.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "student_id": {
                                "type": "string",
                                "description": "í•™ìƒ ID (CEFR ë ˆë²¨ ìë™ ì¡°íšŒìš©)"
                            },
                            "area": {
                                "type": "string",
                                "description": "ë¬¸ì œ ì˜ì—­: 'ë“£ê¸°' (listening), 'ë…í•´' (reading), 'ë¬¸ë²•' (grammar), 'ì–´íœ˜' (vocabulary), 'ì“°ê¸°' (writing)"
                            },
                            "difficulty": {
                                "type": "string",
                                "description": "ë‚œì´ë„ CEFR ë ˆë²¨ (A1, A2, B1, B2, C1, C2). ìƒëµ ì‹œ í•™ìƒì˜ í˜„ì¬ ë ˆë²¨ ì‚¬ìš©"
                            },
                            "topic": {
                                "type": "string",
                                "description": "ì£¼ì œ - ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì£¼ì œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: ì—¬í–‰, ì‡¼í•‘, ë ˆìŠ¤í† ë‘ ì˜ˆì•½, ë³‘ì› ì˜ˆì•½, ë„ì„œê´€, ì˜í™”ê´€, ìš´ë™, ì·¨ë¯¸, í•™êµ ìƒí™œ, ì¹œêµ¬ ëª¨ì„, ê°€ì¡± í–‰ì‚¬, íœ´ê°€ ê³„íš, ë´‰ì‚¬í™œë™ ë“±. ì–¸ê¸‰ì´ ì—†ìœ¼ë©´ ë‹¤ì–‘í•œ ì£¼ì œ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”."
                            },
                            "num_speakers": {
                                "type": "integer",
                                "description": "ë“£ê¸° ë¬¸ì œì˜ í™”ì ìˆ˜ (2, 3, 4 ë“±). ì‚¬ìš©ìê°€ '3ëª…', 'ì—¬ëŸ¬ ëª…', 'ì„¸ ëª…' ë“±ì„ ì–¸ê¸‰í•˜ë©´ í•´ë‹¹ ìˆ«ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ê¸°ë³¸ê°’ì€ 2ëª…ì…ë‹ˆë‹¤."
                            }
                        },
                        "required": ["student_id", "area"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "evaluate_writing",
                    "description": "ì“°ê¸°(ì„œìˆ í˜•) ë‹µì•ˆì„ AIê°€ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤ (ë¬¸ë²•, ì–´íœ˜, êµ¬ì¡°, ë‚´ìš©)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "prompt": {
                                "type": "string",
                                "description": "ì“°ê¸° ë¬¸ì œ í”„ë¡¬í”„íŠ¸ (ì£¼ì œ/ì§ˆë¬¸)"
                            },
                            "student_answer": {
                                "type": "string",
                                "description": "í•™ìƒì´ ì‘ì„±í•œ ì˜ì–´ ë‹µì•ˆ"
                            },
                            "difficulty": {
                                "type": "string",
                                "description": "ë‚œì´ë„ CEFR ë ˆë²¨ (A1, A2, B1, B2, C1, C2)"
                            }
                        },
                        "required": ["prompt", "student_answer", "difficulty"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "lookup_word",
                    "description": "ì˜ì–´ ë‹¨ì–´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë°œìŒ, ì •ì˜, ì˜ˆë¬¸, ë™ì˜ì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤ (Free Dictionary API, ë¬´ë£Œ)",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "word": {
                                "type": "string",
                                "description": "ê²€ìƒ‰í•  ì˜ì–´ ë‹¨ì–´ (ì˜ˆ: confident, beautiful, education)"
                            }
                        },
                        "required": ["word"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "fetch_news",
                    "description": "ì˜ì–´ í•™ìŠµìš© ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (NewsAPI). ë‰´ìŠ¤ë¥¼ ì½ê³  ì‹¶ê±°ë‚˜, ìµœì‹  ì£¼ì œë¡œ í•™ìŠµí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "category": {
                                "type": "string",
                                "description": "ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬: general(ì¼ë°˜), sports(ìŠ¤í¬ì¸ ), technology(ê¸°ìˆ ), health(ê±´ê°•), science(ê³¼í•™), entertainment(ì—”í„°)",
                                "enum": ["general", "sports", "technology", "health", "science", "entertainment"],
                                "default": "general"
                            },
                            "page_size": {
                                "type": "integer",
                                "description": "ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (1-10, ê¸°ë³¸ê°’ 3)",
                                "default": 3,
                                "minimum": 1,
                                "maximum": 10
                            }
                        },
                        "required": []
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "analyze_text_difficulty",
                    "description": "ì˜ì–´ í…ìŠ¤íŠ¸ì˜ ë‚œì´ë„(CEFR ë ˆë²¨)ì™€ ê°€ë…ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤. í•™ìƒì´ ì‘ì„±í•œ ê¸€ì´ë‚˜ ì½ì€ ì§€ë¬¸ì˜ ìˆ˜ì¤€ì„ íŒŒì•…í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "text": {
                                "type": "string",
                                "description": "ë¶„ì„í•  ì˜ì–´ í…ìŠ¤íŠ¸ (ìµœì†Œ 1ë¬¸ì¥ ì´ìƒ)"
                            }
                        },
                        "required": ["text"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "check_grammar",
                    "description": "ì˜ì–´ ë¬¸ì¥ì˜ ë¬¸ë²• ì˜¤ë¥˜ë¥¼ ìë™ìœ¼ë¡œ ê²€ì‚¬í•˜ê³  ìˆ˜ì • ì œì•ˆì„ ì œê³µí•©ë‹ˆë‹¤ (LanguageTool API, ë¬´ë£Œ). í•™ìƒì´ ì‘ì„±í•œ ë¬¸ì¥ì„ ê²€í† í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "text": {
                                "type": "string",
                                "description": "ê²€ì‚¬í•  ì˜ì–´ ë¬¸ì¥ ë˜ëŠ” ë‹¨ë½"
                            }
                        },
                        "required": ["text"]
                    }
                }
            }
        ]

    def _get_student_context(self, student_id: str, query_text: str = "í•™ìƒ ì •ë³´ ì¡°íšŒ") -> str:
        """í•™ìƒ ì •ë³´ ì¡°íšŒ ì‹¤í–‰ (ë²¡í„° ê²€ìƒ‰ í¬í•¨)"""
        try:
            context = self.graph_rag_service.get_rag_context(
                student_id=student_id,
                query_text=query_text,
                use_vector_search=True
            )
            return context
        except Exception as e:
            return f"í•™ìƒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

    def _recommend_problems(self, student_id: str, area: Optional[str] = None, limit: int = 3) -> str:
        """ë¬¸ì œ ì¶”ì²œ ì‹¤í–‰"""
        try:
            # Writing(ì„œìˆ í˜•)ì€ DBì—ì„œ ì¶”ì²œí•˜ì§€ ì•ŠìŒ - í•­ìƒ generate_problem ì‚¬ìš©
            if area and area.upper() in ['WR', 'WRITING', 'ì“°ê¸°']:
                return "ì“°ê¸°(Writing) ë¬¸ì œëŠ” DB ì¶”ì²œì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. generate_problem functionì„ ì‚¬ìš©í•˜ì—¬ ììœ  ì„œìˆ í˜• ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."

            problems = self.graph_rag_service.search_problems_for_student(
                student_id=student_id,
                area=area,
                limit=limit
            )

            if not problems:
                return "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ë¬¸ì œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            result = []
            area_names = {
                'RD': 'Reading', 'GR': 'Grammar', 'WR': 'Writing',
                'LS': 'Listening', 'VO': 'Vocabulary'
            }

            for i, p in enumerate(problems, 1):
                area_en = area_names.get(p['area'], p['area'])
                problem_text = f"Problem {i} [{area_en}]:\n"

                # Listening ë¬¸ì œëŠ” audio_transcript í¬í•¨
                if p.get('audio_transcript'):
                    problem_text += f"[AUDIO]: {p['audio_transcript']}\n"

                # Figure (ê·¸ë¦¼) í¬í•¨
                if p.get('figures'):
                    for fig in p['figures']:
                        if fig.get('public_url'):
                            problem_text += f"[IMAGE]: {fig['public_url']}\n"
                            if fig.get('caption'):
                                problem_text += f"  Caption: {fig['caption']}\n"

                # Table (í‘œ) í¬í•¨
                if p.get('tables'):
                    for tbl in p['tables']:
                        if tbl.get('public_url'):
                            problem_text += f"[TABLE]: {tbl['public_url']}\n"
                            if tbl.get('title'):
                                problem_text += f"  Title: {tbl['title']}\n"

                problem_text += f"{p['stem']}\n"
                if p['options']:
                    for j, opt in enumerate(p['options'], 1):
                        problem_text += f"   {j}) {opt}\n"
                problem_text += f"Answer: {p['answer']}\n"
                result.append(problem_text)

            return "\n".join(result)
        except Exception as e:
            return f"ë¬¸ì œ ì¶”ì²œ ì‹¤íŒ¨: {str(e)}"

    def _generate_problem(self, student_id: str, area: str, difficulty: str = None, topic: str = None, num_speakers: int = 2) -> str:
        """AI ë¬¸ì œ ìƒì„± ì‹¤í–‰ (o4-mini - ë¹ ë¥¸ ì¶”ë¡  ëª¨ë¸)"""
        try:
            # topicì´ ì—†ìœ¼ë©´ ë‹¤ì–‘í•œ ì£¼ì œ ì¤‘ ëœë¤ ì„ íƒ
            if not topic:
                import random
                topics = [
                    "ë ˆìŠ¤í† ë‘ ì˜ˆì•½", "ì˜í™”ê´€ ë°©ë¬¸", "ë„ì„œê´€ ì´ìš©", "ì‡¼í•‘", "ë³‘ì› ì˜ˆì•½",
                    "ì—¬í–‰ ê³„íš", "ìš´ë™", "ì·¨ë¯¸ í™œë™", "í•™êµ ìƒí™œ", "ì¹œêµ¬ ëª¨ì„",
                    "ê°€ì¡± í–‰ì‚¬", "íœ´ê°€ ê³„íš", "ë´‰ì‚¬í™œë™", "ë™ì•„ë¦¬ í™œë™", "íŒŒí‹° ì¤€ë¹„"
                ]
                topic = random.choice(topics)
                print(f"ğŸ“Œ ìë™ ì„ íƒëœ ì£¼ì œ: {topic}")

            # difficultyê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ í•™ìƒì˜ CEFR ë ˆë²¨ ì¡°íšŒ
            if not difficulty:
                try:
                    # GraphRAG ì„œë¹„ìŠ¤ë¥¼ í†µí•´ í•™ìƒ ì •ë³´ ì¡°íšŒ
                    student_context = self.graph_rag_service.get_rag_context(
                        student_id=student_id,
                        query_text="í•™ìƒì˜ CEFR ë ˆë²¨ ì•Œë ¤ì¤˜",
                        use_vector_search=False  # ì§ì ‘ ê·¸ë˜í”„ ì¡°íšŒ
                    )

                    # CEFR ë ˆë²¨ ì¶”ì¶œ (A1, A2, B1, B2, C1, C2 ì¤‘ í•˜ë‚˜)
                    import re
                    cefr_match = re.search(r'\b(A1|A2|B1|B2|C1|C2)\b', student_context, re.IGNORECASE)
                    if cefr_match:
                        difficulty = cefr_match.group(1).upper()
                        print(f"ğŸ“Š í•™ìƒ {student_id}ì˜ CEFR ë ˆë²¨: {difficulty}")
                    else:
                        # ê¸°ë³¸ê°’: B1
                        difficulty = "B1"
                        print(f"âš ï¸  í•™ìƒ {student_id}ì˜ CEFR ë ˆë²¨ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ B1 ì‚¬ìš©")
                except Exception as e:
                    print(f"âš ï¸  CEFR ë ˆë²¨ ì¡°íšŒ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ B1 ì‚¬ìš©")
                    difficulty = "B1"

            # PromptManagerë¥¼ ì‚¬ìš©í•´ ë¬¸ì œ ìƒì„± í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            prompt = PromptManager.get_problem_generation_prompt(
                area=area,
                difficulty=difficulty,
                topic=topic,
                num_speakers=num_speakers,
                model="o4-mini"
            )

            is_listening = area.lower() in ['ë“£ê¸°', 'listening', 'ls']
            print(f"ğŸ¯ ë¬¸ì œ ìƒì„±: area={area}, difficulty={difficulty}, topic={topic}, num_speakers={num_speakers}, is_listening={is_listening}")

            # ë“£ê¸° ë¬¸ì œëŠ” ìµœëŒ€ 2íšŒ ì¬ì‹œë„
            max_attempts = 2 if is_listening else 1

            for attempt in range(max_attempts):
                try:
                    # o4-mini - o3-mini í›„ì†, ë¹ ë¥¸ ì†ë„ + ìš°ìˆ˜í•œ STEM ì„±ëŠ¥
                    # NOTE: o4-mini uses reasoning tokens + output tokens, so we need more tokens for listening problems
                    max_tokens = 10000 if is_listening else 3000
                    response = self.client.chat.completions.create(
                        model="o4-mini",
                        messages=[{"role": "user", "content": prompt}],
                        max_completion_tokens=max_tokens
                    )

                    content = response.choices[0].message.content

                    if is_listening and content:
                        print(f"âœ… o4-mini ë“£ê¸° ë¬¸ì œ ìƒì„± ì™„ë£Œ: {len(content)} ê¸€ì")

                    # ë“£ê¸° ë¬¸ì œ í›„ì²˜ë¦¬
                    if is_listening:
                        content = self._postprocess_listening_problem(content, attempt + 1)

                    return content

                except Exception as retry_error:
                    if attempt == max_attempts - 1:
                        raise retry_error
                    print(f"âš ï¸  ë“£ê¸° ë¬¸ì œ ìƒì„± ì¬ì‹œë„ ({attempt + 1}/{max_attempts})")
                    continue

        except Exception as e:
            # Fallback to GPT-4o (ë¹ ë¥´ê³  ì•ˆì •ì )
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "You are an expert English language teacher creating high-quality assessment questions."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.8,
                    max_tokens=1500
                )

                content = response.choices[0].message.content

                # ë“£ê¸° ë¬¸ì œ í›„ì²˜ë¦¬
                if is_listening:
                    content = self._postprocess_listening_problem(content, attempt=1)

                return content

            except Exception as fallback_error:
                return f"ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {str(e)}, Fallback ì‹¤íŒ¨: {str(fallback_error)}"

    def _postprocess_listening_problem(self, content: str, attempt: int) -> str:
        """
        ë“£ê¸° ë¬¸ì œ í›„ì²˜ë¦¬ (ê°•ì œ ê²€ì¦ ë° ìˆ˜ì •)

        1. í•œê¸€ í…ìŠ¤íŠ¸ ì œê±° (TTSì—ì„œ ì½íˆì§€ ì•Šë„ë¡)
        2. [AUDIO]: íŒ¨í„´ í™•ì¸ ë° ì¶”ê°€
        3. [SPEAKERS]: JSON íŒŒì‹± ë° ìë™ ìƒì„±
        4. ëŒ€í™” í˜•ì‹ ê²€ì¦
        5. ì²« ë°œí™”ì— í™”ì ì´ë¦„ ì¶”ê°€
        """
        import re
        import json

        # ========================================
        # STEP 1: í•œê¸€ í…ìŠ¤íŠ¸ ì œê±° (TTS ìŒì„± ë°©ì§€)
        # ========================================
        # 1. ê´„í˜¸ ì•ˆì˜ í•œê¸€ ë²ˆì—­ ì œê±° (ì˜ˆ: "(ì•ˆë…•, íŒŒí‹°ì— ì˜¬ ê±°ì•¼?)" â†’ "")
        # 2. ì™„ì „íˆ í•œê¸€ë¡œë§Œ ëœ ì„¤ëª… ì¤„ë§Œ ì œê±°
        # 3. ì˜ì–´ ëŒ€í™”ëŠ” ìœ ì§€

        korean_char_pattern = re.compile(r'[\u3131-\u3163\uac00-\ud7a3]')  # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„

        cleaned_lines = []
        for line in content.split('\n'):
            # ê´„í˜¸ ì•ˆì˜ í•œê¸€ ë²ˆì—­ ì œê±° (ì˜ˆ: "Hello! (ì•ˆë…•!) How are you? (ì–´ë–»ê²Œ ì§€ë‚´?)" â†’ "Hello! How are you?")
            cleaned_line = re.sub(r'\([^)]*[\u3131-\u3163\uac00-\ud7a3][^)]*\)', '', line)
            # ì—°ì†ëœ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì •ë¦¬
            cleaned_line = re.sub(r'\s+', ' ', cleaned_line).strip()

            # ì™„ì „íˆ í•œê¸€ë¡œë§Œ ëœ ì¤„ë§Œ ì œê±° (ì„¤ëª… ì¤„)
            # ì˜ˆ: "ìŒì„± ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ ë“£ê¸° ì—°ìŠµ", "[ìŒì„± ë‚´ìš© ìš”ì•½]", "- ê³µì› ì²­ì†Œ ë´‰ì‚¬ í™œë™ì´..."
            if re.match(r'^[\s\u3131-\u3163\uac00-\ud7a3\[\]:\-â€¢â€»\(\)]+$', cleaned_line):
                print(f"   ğŸ—‘ï¸  í•œê¸€ ì„¤ëª… ì¤„ ì œê±°: {line[:50]}...")
                continue

            # ë¹ˆ ì¤„ì€ ìœ ì§€ (êµ¬ì¡° ë³´ì¡´)
            cleaned_lines.append(cleaned_line if cleaned_line else line)

        content = '\n'.join(cleaned_lines)
        print(f"   âœ… í•œê¸€ í…ìŠ¤íŠ¸ ì œê±° ì™„ë£Œ (ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ìœ ì§€)")

        lines = content.split('\n')

        # 1. [AUDIO]: íŒ¨í„´ í™•ì¸
        has_audio_tag = any('[AUDIO]:' in line for line in lines)

        # 2. [SPEAKERS]: JSON íŒŒì‹±
        speakers_match = re.search(r'\[SPEAKERS\]:\s*({.*})', content)
        has_speakers_tag = speakers_match is not None

        # Check if voice field exists in existing SPEAKERS JSON
        needs_voice_enhancement = False
        if has_speakers_tag:
            try:
                existing_speakers = json.loads(speakers_match.group(1))
                speakers_list = existing_speakers.get('speakers', [])
                # Check if ANY speaker is missing voice field
                if speakers_list and not all('voice' in s for s in speakers_list):
                    needs_voice_enhancement = True
                    print(f"   âš ï¸  [SPEAKERS]: voice í•„ë“œ ì—†ìŒ â†’ o4-minië¡œ ì¶”ê°€ ì˜ˆì •")
            except:
                pass

        # 3. ëŒ€í™” í˜•ì‹ í™•ì¸ (Name: text íŒ¨í„´)
        dialogue_pattern = re.compile(r'^([A-Z][a-z]+):\s+.+', re.MULTILINE)
        dialogue_matches = dialogue_pattern.findall(content)

        # ì²« ë²ˆì§¸ ëŒ€í™” ì°¾ê¸° (í™”ì ì—†ì´ ì‹œì‘í•˜ëŠ” ê²½ìš°ë„ í¬í•¨)
        # ì˜ˆ: "Hi Jake, have you done..." ê°™ì€ ê²½ìš°
        first_utterance_pattern = re.compile(r'^[A-Z][^:]{10,}', re.MULTILINE)
        first_utterance_matches = first_utterance_pattern.findall(content)

        has_dialogue = len(dialogue_matches) >= 1 or len(first_utterance_matches) >= 1

        print(f"ğŸ” ë“£ê¸° ë¬¸ì œ ê²€ì¦ (ì‹œë„ {attempt}):")
        print(f"   - [AUDIO]: {has_audio_tag}")
        print(f"   - [SPEAKERS]: {has_speakers_tag}")
        print(f"   - ëŒ€í™” í˜•ì‹ (Name:): {len(dialogue_matches)}ê°œ")
        print(f"   - ì²« ë°œí™” (í™”ì ì—†ìŒ): {len(first_utterance_matches)}ê°œ")

        # ëŒ€í™” í˜•ì‹ì´ ì—†ìœ¼ë©´ ë…ë°±í˜•ìœ¼ë¡œ ê°„ì£¼ (í™”ì ë¶„ë¦¬ ì—†ìŒ)
        if not has_dialogue:
            print(f"   â„¹ï¸  ë…ë°±í˜• ë“£ê¸° ë¬¸ì œ (í™”ì ë¶„ë¦¬ ì—†ìŒ)")
            print(f"   âœ… ë“£ê¸° ë¬¸ì œ ê²€ì¦ í†µê³¼ (ë…ë°±í˜•)!")
            return '\n'.join(lines)  # í›„ì²˜ë¦¬ ì—†ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜

        # [AUDIO]: íƒœê·¸ ê°•ì œ ì¶”ê°€
        if not has_audio_tag:
            print(f"   âš ï¸  [AUDIO]: íƒœê·¸ ì—†ìŒ â†’ ê°•ì œ ì¶”ê°€")
            # Find first dialogue line or first utterance
            first_dialogue_idx = None
            for i, line in enumerate(lines):
                if dialogue_pattern.match(line.strip()) or first_utterance_pattern.match(line.strip()):
                    first_dialogue_idx = i
                    break

            if first_dialogue_idx is not None:
                lines.insert(first_dialogue_idx, '[AUDIO]:')

        # [SPEAKERS]: JSON ìë™ ìƒì„± ë˜ëŠ” voice enhancement
        if (not has_speakers_tag or needs_voice_enhancement) and has_dialogue:
            if not has_speakers_tag:
                print(f"   âš ï¸  [SPEAKERS]: íƒœê·¸ ì—†ìŒ â†’ ìë™ ìƒì„±")
            else:
                print(f"   âš ï¸  [SPEAKERS]: voice í•„ë“œ ì¶”ê°€ ì¤‘...")

            # Extract unique speaker names from existing labels
            unique_speakers = []
            seen = set()
            for match in dialogue_matches:
                if match not in seen:
                    unique_speakers.append(match)
                    seen.add(match)

            # If only 1 speaker found but we need 2, add a default second speaker
            if len(unique_speakers) == 1:
                # Infer second speaker from context
                first_name = unique_speakers[0]
                # Default second names
                if first_name.lower() in ['emma', 'sarah', 'lisa', 'maria', 'anna']:
                    second_name = 'Jake'  # Male counterpart
                else:
                    second_name = 'Emma'  # Female counterpart
                unique_speakers.insert(0, second_name)  # Add as first speaker
                print(f"   â„¹ï¸  í™”ì 1ëª…ë§Œ ê°ì§€ â†’ ë‘ ë²ˆì§¸ í™”ì ì¶”ê°€: {second_name}")

            # Use LLM to determine gender and voice for each speaker
            # Build a prompt for the LLM
            speaker_names = ", ".join(unique_speakers)
            llm_prompt = f"""Given these speaker names: {speaker_names}

For EACH speaker, determine:
1. Gender (male or female)
2. US English voice name - IMPORTANT: Assign DIFFERENT voices to DIFFERENT speakers!

Available voices:
- Female voices: Samantha, Karen, Victoria
- Male voices: David, Daniel, Mark

CRITICAL RULES:
- If you have 2 speakers, they MUST use DIFFERENT voice names
- Never assign the same voice to multiple speakers in one conversation
- Example (CORRECT): Speaker1 uses Samantha, Speaker2 uses David
- Example (WRONG): Speaker1 uses Samantha, Speaker2 uses Samantha

Respond with ONLY valid JSON:
{{
  "speakers": [
    {{"name": "Name1", "gender": "female", "voice": "Samantha"}},
    {{"name": "Name2", "gender": "male", "voice": "David"}}
  ]
}}"""

            try:
                # Call LLM to determine speakers (o4-mini - ì¶”ë¡  ëª¨ë¸)
                llm_response = self.client.chat.completions.create(
                    model="o4-mini",  # Reasoning model for speaker analysis
                    messages=[{"role": "user", "content": llm_prompt}],
                    max_completion_tokens=300
                )

                # o4-mini doesn't support JSON mode, parse from text
                content = llm_response.choices[0].message.content
                # Extract JSON from response (handle markdown code blocks)
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    speakers_json = json.loads(json_match.group(0))
                else:
                    speakers_json = json.loads(content)

                print(f"   âœ… o4-miniê°€ í™”ì ì •ë³´ ìƒì„±: {speakers_json}")

            except Exception as e:
                print(f"   âš ï¸  LLM í˜¸ì¶œ ì‹¤íŒ¨, fallback to rules: {e}")
                # Fallback to rule-based
                female_names = {'sarah', 'emma', 'lisa', 'maria', 'anna', 'kate', 'jane', 'mary', 'lucy', 'emily'}
                male_names = {'tom', 'john', 'mike', 'david', 'james', 'robert', 'mark', 'peter', 'kevin', 'alex', 'jake'}

                speakers_json = {"speakers": []}
                for name in unique_speakers:
                    name_lower = name.lower()
                    if name_lower in female_names:
                        gender = "female"
                        voice = "Samantha"
                    elif name_lower in male_names:
                        gender = "male"
                        voice = "David"
                    else:
                        # Default: alternate male/female
                        gender = "female" if len(speakers_json["speakers"]) % 2 == 0 else "male"
                        voice = "Samantha" if gender == "female" else "David"

                    speakers_json["speakers"].append({"name": name, "gender": gender, "voice": voice})

            # Insert or Replace [SPEAKERS]: line
            speakers_line = f"[SPEAKERS]: {json.dumps(speakers_json)}"

            if needs_voice_enhancement:
                # Replace existing [SPEAKERS]: line
                for i, line in enumerate(lines):
                    if '[SPEAKERS]:' in line:
                        lines[i] = speakers_line
                        print(f"   âœ… [SPEAKERS]: voice í•„ë“œ ì¶”ê°€ ì™„ë£Œ ({len(speakers_json['speakers'])}ëª…)")
                        break
            else:
                # Insert new [SPEAKERS]: line after [AUDIO]:
                audio_idx = None
                for i, line in enumerate(lines):
                    if '[AUDIO]:' in line:
                        audio_idx = i
                        break

                if audio_idx is not None:
                    lines.insert(audio_idx + 1, speakers_line)
                    print(f"   âœ… [SPEAKERS]: ìë™ ìƒì„± ì™„ë£Œ ({len(speakers_json['speakers'])}ëª…)")

        # 4. ì²« ë°œí™”ì— í™”ì ì´ë¦„ ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
        # Re-parse speakers JSON to get first speaker
        speakers_match = re.search(r'\[SPEAKERS\]:\s*({.*})', '\n'.join(lines))
        if speakers_match:
            try:
                speakers_data = json.loads(speakers_match.group(1))
                speakers_list = speakers_data.get('speakers', [])

                if speakers_list:
                    first_speaker = speakers_list[0]['name']

                    # Find first line after [SPEAKERS]: that looks like dialogue but has no speaker label
                    speakers_idx = None
                    for i, line in enumerate(lines):
                        if '[SPEAKERS]:' in line:
                            speakers_idx = i
                            break

                    if speakers_idx is not None:
                        # Check next few lines for dialogue without speaker
                        for i in range(speakers_idx + 1, min(speakers_idx + 5, len(lines))):
                            line = lines[i].strip()
                            # Skip empty lines and lines that already have speaker labels
                            if not line or dialogue_pattern.match(line):
                                continue
                            # If it's a sentence (starts with capital, has words)
                            if re.match(r'^[A-Z][^:]{10,}', line):
                                # Add first speaker name
                                lines[i] = f"{first_speaker}: {line}"
                                print(f"   âœ… ì²« ë°œí™”ì— í™”ì ì¶”ê°€: {first_speaker}:")
                                break
            except Exception as e:
                print(f"   âš ï¸  ì²« ë°œí™” í™”ì ì¶”ê°€ ì‹¤íŒ¨: {e}")

        result = '\n'.join(lines)

        # Final verification
        final_has_audio = '[AUDIO]:' in result
        final_has_speakers = '[SPEAKERS]:' in result
        final_dialogue_count = len(re.findall(r'^([A-Z][a-z]+):\s+', result, re.MULTILINE))

        print(f"   ğŸ“Š ìµœì¢… ëŒ€í™” ìˆ˜: {final_dialogue_count}ê°œ")

        if final_has_audio and final_has_speakers and final_dialogue_count >= 2:
            print(f"   âœ… ë“£ê¸° ë¬¸ì œ ê²€ì¦ í†µê³¼!")
        else:
            print(f"   âš ï¸  ìµœì¢… ê²€ì¦ - AUDIO: {final_has_audio}, SPEAKERS: {final_has_speakers}, ëŒ€í™”: {final_dialogue_count}ê°œ")

        # ğŸ™ï¸ OpenAI TTSë¡œ ê³ í’ˆì§ˆ ìŒì„± ìƒì„±
        if final_has_audio and final_has_speakers:
            try:
                print(f"   ğŸ™ï¸  OpenAI TTS ìŒì„± ìƒì„± ì¤‘...")
                tts_service = get_tts_service()
                audio_url = tts_service.get_or_create_audio(result, session_id=self.current_session_id)

                if audio_url:
                    # Add audio URL to the beginning of the problem (for frontend to use)
                    result = f"[AUDIO_URL]: {audio_url}\n\n{result}"
                    print(f"   âœ… TTS ìŒì„± ìƒì„± ì™„ë£Œ: {audio_url}")
                else:
                    print(f"   âš ï¸  TTS ìŒì„± ìƒì„± ì‹¤íŒ¨ - ë¸Œë¼ìš°ì € TTS ì‚¬ìš©")
            except Exception as e:
                print(f"   âš ï¸  TTS ì˜¤ë””ì˜¤ ìƒì„± ì˜¤ë¥˜: {e}")
                print(f"   â„¹ï¸  ë¸Œë¼ìš°ì € TTS fallback ì‚¬ìš©")

        return result

    def _evaluate_writing(self, prompt: str, student_answer: str, difficulty: str) -> str:
        """
        ì„œìˆ í˜• ì“°ê¸° ë‹µì•ˆ í‰ê°€ (o4-mini ì¶”ë¡  ëª¨ë¸)

        í‰ê°€ ê¸°ì¤€:
        - Grammar (ë¬¸ë²•): 15ì 
        - Vocabulary (ì–´íœ˜): 15ì 
        - Organization (êµ¬ì¡°): 20ì 
        - Content (ë‚´ìš©): 30ì 
        - Fluency (ìœ ì°½ì„±): 20ì 
        ì´ 100ì 
        """
        try:
            evaluation_prompt = f"""You are an expert English writing evaluator for CEFR {difficulty} level students.

**Writing Prompt:**
{prompt}

**Student's Answer:**
{student_answer}

**Evaluation Task:**
Evaluate this student's writing comprehensively using the following criteria:

1. **Grammar (15 points)**: Accuracy of grammar, sentence structure, verb tenses
2. **Vocabulary (15 points)**: Range and appropriateness of vocabulary for {difficulty} level
3. **Organization (20 points)**: Logical flow, paragraph structure, coherence
4. **Content (30 points)**: Relevance to prompt, depth of ideas, completeness
5. **Fluency (20 points)**: Natural expression, readability, overall language flow

**Output Format (IMPORTANT):**
Provide your evaluation in the following structured format:

**Overall Score:** [X/100]

**Grammar (X/15):**
- Strengths: [brief]
- Weaknesses: [brief]

**Vocabulary (X/15):**
- Strengths: [brief]
- Weaknesses: [brief]

**Organization (X/20):**
- Strengths: [brief]
- Weaknesses: [brief]

**Content (X/30):**
- Strengths: [brief]
- Weaknesses: [brief]

**Fluency (X/20):**
- Strengths: [brief]
- Weaknesses: [brief]

**Key Recommendations:**
1. [First improvement suggestion]
2. [Second improvement suggestion]
3. [Third improvement suggestion]

**Corrected Version (if needed):**
[Provide a corrected/improved version of the student's writing, maintaining their ideas but fixing errors]
"""

            # o4-minië¡œ ì¶”ë¡  ê¸°ë°˜ í‰ê°€
            response = self.client.chat.completions.create(
                model="o4-mini",
                messages=[{"role": "user", "content": evaluation_prompt}],
                max_completion_tokens=2500
            )

            return response.choices[0].message.content

        except Exception as e:
            # Fallback to GPT-4o
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "You are an expert English writing evaluator."},
                        {"role": "user", "content": evaluation_prompt}
                    ],
                    temperature=0.3,  # Lower temperature for consistent evaluation
                    max_tokens=2000
                )
                return response.choices[0].message.content
            except Exception as fallback_error:
                return f"í‰ê°€ ì‹¤íŒ¨: {str(e)}, Fallback ì‹¤íŒ¨: {str(fallback_error)}"

    def _lookup_word(self, word: str) -> str:
        """ì˜ì–´ ë‹¨ì–´ ê²€ìƒ‰ (Free Dictionary API)"""
        try:
            service = get_dictionary_service()
            result = service.lookup_word(word)

            if not result.get('success'):
                return f"ë‹¨ì–´ '{word}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì² ìë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

            # ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ í¬ë§·íŒ…
            response = f"**{result['word']}** {result.get('phonetic', '')}\n\n"
            response += f"**í’ˆì‚¬:** {result.get('part_of_speech', 'N/A')}\n\n"
            response += f"**ì •ì˜:** {result.get('definition', 'N/A')}\n\n"

            if result.get('example'):
                response += f"**ì˜ˆë¬¸:** {result['example']}\n\n"

            if result.get('synonyms'):
                synonyms = ', '.join(result['synonyms'])
                response += f"**ë™ì˜ì–´:** {synonyms}\n\n"

            if result.get('antonyms'):
                antonyms = ', '.join(result['antonyms'])
                response += f"**ë°˜ì˜ì–´:** {antonyms}\n\n"

            # ë°œìŒ ì˜¤ë””ì˜¤ URLì´ ìˆìœ¼ë©´ ì¶”ê°€
            phonetics = result.get('phonetics', [])
            audio_urls = [p['audio'] for p in phonetics if p.get('audio')]
            if audio_urls:
                response += f"**ë°œìŒ ë“£ê¸°:** {audio_urls[0]}\n"

            return response
        except Exception as e:
            return f"ë‹¨ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"

    def _fetch_news(self, category: str = "general", page_size: int = 3) -> str:
        """ì˜ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰ (NewsAPI)"""
        try:
            service = get_news_service()
            result = service.fetch_news(
                category=category,
                language="en",
                page_size=min(page_size, 5),  # ìµœëŒ€ 5ê°œ
                country="us"
            )

            if not result.get('success'):
                error_msg = result.get('error', 'Unknown error')
                if "NEWS_API_KEY not configured" in error_msg:
                    return "ë‰´ìŠ¤ APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
                return f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {error_msg}"

            articles = result.get('articles', [])
            if not articles:
                return f"'{category}' ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ê¸°ì‚¬ í¬ë§·íŒ…
            response = f"**{category.title()} ì¹´í…Œê³ ë¦¬ ìµœì‹  ë‰´ìŠ¤** ({len(articles)}ê°œ)\n\n"

            for i, article in enumerate(articles, 1):
                response += f"**{i}. {article['title']}**\n"
                response += f"   *ì¶œì²˜:* {article['source']}\n"
                if article.get('description'):
                    response += f"   *ìš”ì•½:* {article['description']}\n"
                response += f"   *ë§í¬:* {article['url']}\n\n"

            return response
        except Exception as e:
            return f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"

    def _analyze_text_difficulty(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„ (textstat)"""
        try:
            service = get_text_analysis_service()
            result = service.analyze_cefr_level(text)

            if not result.get('success'):
                error_msg = result.get('error', 'Unknown error')
                if "textstat library not installed" in error_msg:
                    return "í…ìŠ¤íŠ¸ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
                return f"í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {error_msg}"

            # ê²°ê³¼ í¬ë§·íŒ…
            response = f"**í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„ ê²°ê³¼**\n\n"
            response += f"**CEFR ë ˆë²¨:** {result['cefr_level']}\n"
            response += f"**ë‚œì´ë„:** {result['difficulty']}\n"
            response += f"**ê°€ë…ì„± ì ìˆ˜ (Flesch):** {result['flesch_reading_ease']}/100 (ë†’ì„ìˆ˜ë¡ ì‰¬ì›€)\n"
            response += f"**í•™ë…„ ìˆ˜ì¤€ (FK Grade):** {result['flesch_kincaid_grade']}í•™ë…„\n\n"
            response += f"**í†µê³„:**\n"
            response += f"- ë‹¨ì–´ ìˆ˜: {result['word_count']}ê°œ\n"
            response += f"- ë¬¸ì¥ ìˆ˜: {result['sentence_count']}ê°œ\n"
            response += f"- í‰ê·  ë¬¸ì¥ ê¸¸ì´: {result['avg_sentence_length']}ë‹¨ì–´\n\n"

            # ë ˆë²¨ë³„ ì„¤ëª…
            level_descriptions = {
                "A1": "ì´ˆê¸‰ - ë§¤ìš° ì‰¬ìš´ í…ìŠ¤íŠ¸",
                "A2": "ì´ˆê¸‰ ìƒ - ì‰¬ìš´ í…ìŠ¤íŠ¸",
                "B1": "ì¤‘ê¸‰ - ë³´í†µ ë‚œì´ë„ í…ìŠ¤íŠ¸",
                "B2": "ì¤‘ê¸‰ ìƒ - ì–´ë ¤ìš´ í…ìŠ¤íŠ¸",
                "C1": "ê³ ê¸‰ - ë§¤ìš° ì–´ë ¤ìš´ í…ìŠ¤íŠ¸",
                "C2": "ê³ ê¸‰ ìƒ - ì „ë¬¸ê°€ ìˆ˜ì¤€ í…ìŠ¤íŠ¸"
            }
            response += f"**ë ˆë²¨ ì„¤ëª…:** {level_descriptions.get(result['cefr_level'], 'N/A')}\n"

            return response
        except Exception as e:
            return f"í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    def _check_grammar(self, text: str) -> str:
        """ë¬¸ë²• ê²€ì‚¬ (LanguageTool API)"""
        try:
            service = get_grammar_check_service()
            result = service.check_grammar(text, language="en-US")

            if not result.get('success'):
                return f"ë¬¸ë²• ê²€ì‚¬ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}"

            error_count = result.get('error_count', 0)
            errors = result.get('errors', [])

            if error_count == 0:
                return "**ë¬¸ë²• ê²€ì‚¬ ê²°ê³¼:** ë¬¸ë²• ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤! ì˜í–ˆì–´ìš”! âœ…"

            # ì˜¤ë¥˜ í¬ë§·íŒ…
            response = f"**ë¬¸ë²• ê²€ì‚¬ ê²°ê³¼:** {error_count}ê°œì˜ ì˜¤ë¥˜ ë°œê²¬\n\n"

            for i, error in enumerate(errors[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                response += f"**{i}. {error['message']}**\n"

                # ì˜¤ë¥˜ ìœ„ì¹˜ í‘œì‹œ
                offset = error['offset']
                length = error['length']
                error_text = text[offset:offset+length]
                response += f"   *ë¬¸ì œ:* \"{error_text}\"\n"

                # ìˆ˜ì • ì œì•ˆ
                replacements = error.get('replacements', [])
                if replacements:
                    suggestions = ', '.join([f'"{r}"' for r in replacements])
                    response += f"   *ì œì•ˆ:* {suggestions}\n"

                # ì¹´í…Œê³ ë¦¬
                category = error.get('category', '')
                if category:
                    response += f"   *ìœ í˜•:* {category}\n"

                response += "\n"

            if error_count > 10:
                response += f"*({error_count - 10}ê°œ ì˜¤ë¥˜ ë” ìˆìŒ)*\n"

            return response
        except Exception as e:
            return f"ë¬¸ë²• ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}"

    def _parse_quick_reply(self, content: str, used_models: List[str]) -> Dict[str, Any]:
        """
        ì‘ë‹µì—ì„œ [QUICK_REPLY:...] íŒ¨í„´ì„ íŒŒì‹±í•˜ì—¬ quick_replies í•„ë“œë¡œ ë³€í™˜

        Format: [QUICK_REPLY:VO|RD|WR|LS|GR]
        ê° ì½”ë“œëŠ” í´ë¦­ ê°€ëŠ¥í•œ ë²„íŠ¼ìœ¼ë¡œ ë³€í™˜ë¨
        """
        import re

        # Quick reply íŒ¨í„´ ê²€ìƒ‰
        pattern = r'\[QUICK_REPLY:(.*?)\]'
        match = re.search(pattern, content)

        if match:
            # Quick reply ì½”ë“œ ì¶”ì¶œ (ì˜ˆ: "VO|RD|WR|LS|GR")
            codes = match.group(1).split('|')

            # ì½”ë“œë¥¼ í•œê¸€ ì´ë¦„ê³¼ ë§¤í•‘
            code_mapping = {
                "VO": {"label": "ì–´íœ˜ (Vocabulary)", "value": "ì–´íœ˜ ë¬¸ì œ ë‚´ì¤˜"},
                "RD": {"label": "ë…í•´ (Reading)", "value": "ë…í•´ ë¬¸ì œ ë‚´ì¤˜"},
                "WR": {"label": "ì“°ê¸° (Writing)", "value": "ì“°ê¸° ë¬¸ì œ ë‚´ì¤˜"},
                "LS": {"label": "ë“£ê¸° (Listening)", "value": "ë“£ê¸° ë¬¸ì œ ë‚´ì¤˜"},
                "GR": {"label": "ë¬¸ë²• (Grammar)", "value": "ë¬¸ë²• ë¬¸ì œ ë‚´ì¤˜"}
            }

            # Quick replies ìƒì„±
            quick_replies = []
            for code in codes:
                code = code.strip()
                if code in code_mapping:
                    quick_replies.append(code_mapping[code])

            # íŒ¨í„´ ì œê±°í•œ ë©”ì‹œì§€
            clean_message = re.sub(pattern, '', content).strip()

            return {
                "message": clean_message,
                "quick_replies": quick_replies,
                "model_info": {
                    "primary": "gpt-4.1-mini",
                    "all_used": used_models
                }
            }
        else:
            # Quick reply ì—†ìœ¼ë©´ ì¼ë°˜ ì‘ë‹µ
            return {
                "message": content,
                "model_info": {
                    "primary": "gpt-4.1-mini",
                    "all_used": used_models
                }
            }

    def _execute_function(self, function_name: str, arguments: Dict[str, Any]) -> str:
        """Function ì‹¤í–‰"""

        # í•¨ìˆ˜ íƒ€ì… ë¶„ë¥˜
        db_functions = ["get_student_context", "recommend_problems"]
        generation_functions = ["generate_problem", "evaluate_writing"]
        external_api_functions = ["lookup_word", "fetch_news", "analyze_text_difficulty", "check_grammar"]

        if function_name in db_functions:
            func_type = "ğŸ“Š DATABASE QUERY"
            color = Colors.GREEN
        elif function_name in generation_functions:
            func_type = "ğŸ¤– AI GENERATION"
            color = Colors.MAGENTA
        elif function_name in external_api_functions:
            func_type = "ğŸŒ EXTERNAL API"
            color = Colors.BLUE
        else:
            func_type = "â“ UNKNOWN"
            color = Colors.RED

        print_section(
            "ğŸ”§",
            f"FUNCTION CALL - {func_type}",
            f"Function: {function_name}\n"
            f"Arguments: {json.dumps(arguments, ensure_ascii=False, indent=2)}",
            color
        )

        if function_name == "get_student_context":
            # ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•´ í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì „ë‹¬
            result = self._get_student_context(query_text=self.current_user_message, **arguments)
        elif function_name == "recommend_problems":
            result = self._recommend_problems(**arguments)
        elif function_name == "generate_problem":
            result = self._generate_problem(**arguments)
        elif function_name == "evaluate_writing":
            result = self._evaluate_writing(**arguments)
        elif function_name == "lookup_word":
            result = self._lookup_word(**arguments)
        elif function_name == "fetch_news":
            result = self._fetch_news(**arguments)
        elif function_name == "analyze_text_difficulty":
            result = self._analyze_text_difficulty(**arguments)
        elif function_name == "check_grammar":
            result = self._check_grammar(**arguments)
        else:
            result = f"Unknown function: {function_name}"

        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥
        result_preview = result[:200] + "..." if len(result) > 200 else result
        print(f"{Colors.GREEN}âœ… Function completed: {result_preview}{Colors.RESET}\n")

        return result

    def chat(
        self,
        student_id: str,
        message: str,
        chat_history: Optional[List[Dict[str, str]]] = None,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        í•™ìƒê³¼ ì±„íŒ… (Function Calling)

        Args:
            student_id: í•™ìƒ ID
            message: í•™ìƒì˜ ë©”ì‹œì§€
            chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒ)
            session_id: ì„¸ì…˜ ID (ì˜¤ë””ì˜¤ ì¶”ì ìš©, ì„ íƒ)

        Returns:
            Dict with 'message' and 'model_info'
        """
        try:
            # ========== ìš”ì²­ ì‹œì‘ ë¡œê¹… ==========
            print_box(
                "ğŸ“ STUDENT CHATBOT REQUEST",
                f"Student ID: {student_id}\n"
                f"Message: {message}\n"
                f"Session ID: {session_id or 'None'}",
                Colors.CYAN
            )

            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ (ë²¡í„° ê²€ìƒ‰ìš©)
            self.current_user_message = message

            # í˜„ì¬ ì„¸ì…˜ ID ì €ì¥ (ì˜¤ë””ì˜¤ ì¶”ì ìš©)
            self.current_session_id = session_id

            # ReAct ëª¨ë“œ íŒë‹¨ (ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì§ˆë¬¸)
            if self._needs_react(message):
                return self._react_chat(student_id, message, chat_history)

            # "ë¬¸ì œ ë‚´ì¤˜" íŒ¨í„´ ê°ì§€ (ìœ í˜• ë¯¸ì§€ì •)
            import re
            if re.search(r'(?:ë¬¸ì œ|problem)\s*(?:ë‚´|ì¤˜|í’€|í•´)', message, re.IGNORECASE) and \
               not re.search(r'(?:ë“£ê¸°|ë…í•´|ë¬¸ë²•|ì–´íœ˜|ì“°ê¸°|listening|reading|grammar|vocabulary|writing|VO|RD|GR|LS|WR)', message, re.IGNORECASE):
                # ìœ í˜•ì´ ëª…ì‹œë˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ "ë¬¸ì œ ë‚´ì¤˜" ìš”ì²­
                print("ğŸ” ë¬¸ì œ ìœ í˜• ì„ íƒ ìš”ì²­ ê°ì§€")
                return {
                    "message": "ì–´ë–¤ ìœ í˜•ì˜ ë¬¸ì œë¥¼ ë‚´ë“œë¦´ê¹Œìš”?",
                    "quick_replies": [
                        {"label": "ğŸ’¬ ë“£ê¸° (Listening)", "value": "ë“£ê¸° ë¬¸ì œ ë‚´ì¤˜"},
                        {"label": "ğŸ“– ë…í•´ (Reading)", "value": "ë…í•´ ë¬¸ì œ ë‚´ì¤˜"},
                        {"label": "âœï¸ ì“°ê¸° (Writing)", "value": "ì“°ê¸° ë¬¸ì œ ë‚´ì¤˜"},
                        {"label": "ğŸ“ ë¬¸ë²• (Grammar)", "value": "ë¬¸ë²• ë¬¸ì œ ë‚´ì¤˜"},
                        {"label": "ğŸ“š ì–´íœ˜ (Vocabulary)", "value": "ì–´íœ˜ ë¬¸ì œ ë‚´ì¤˜"}
                    ],
                    "model_info": {
                        "primary": "gpt-4.1-mini",
                        "all_used": ["gpt-4.1-mini"]
                    }
                }

            used_models = []  # Track models used

            # Step 1: Route the query to determine complexity
            routing_decision = self._route_query(message, student_id)
            used_models.append("gpt-4o-mini")  # Router model

            # Step 2: Select primary model based on routing decision
            if routing_decision == "reasoning":
                primary_model = "o4-mini"
                print(f"ğŸ¯ Using reasoning model: {primary_model}")
            else:
                primary_model = "gpt-4.1-mini"
                print(f"ğŸ¯ Using intelligence model: {primary_model}")

            # PromptManagerë¥¼ ì‚¬ìš©í•´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            system_prompt = PromptManager.get_system_prompt(
                role="student_agent",
                model=primary_model,
                context={"student_id": student_id}
            )

            # ë©”ì‹œì§€ êµ¬ì„± (ë©€í‹°í„´ ì§€ì›)
            messages = [{"role": "system", "content": system_prompt}]

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
            if chat_history:
                messages.extend(chat_history)

            # ìµœì‹  ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            messages.append({"role": "user", "content": message})

            # Step 3: Call primary model with function calling
            used_models.append(primary_model)

            # Use appropriate parameters based on model type
            if primary_model == "o4-mini":
                # o4-mini uses max_completion_tokens
                response = self.client.chat.completions.create(
                    model="o4-mini",
                    messages=messages,
                    tools=self.functions,
                    tool_choice="auto",
                    max_completion_tokens=10000
                )
            else:
                # gpt-4.1-mini uses max_tokens
                response = self.client.chat.completions.create(
                    model="gpt-4.1-mini",
                    messages=messages,
                    tools=self.functions,
                    tool_choice="auto",  # ìë™ìœ¼ë¡œ ë„êµ¬ ì„ íƒ
                    temperature=0.7
                )

            assistant_message = response.choices[0].message

            # Function í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
            if assistant_message.tool_calls:
                # Function ì‹¤í–‰
                messages.append(assistant_message)

                for tool_call in assistant_message.tool_calls:
                    function_name = tool_call.function.name
                    arguments = json.loads(tool_call.function.arguments)

                    print(f"ğŸ”§ Function Call: {function_name}({arguments})")

                    # Track special models used in functions
                    if function_name == "generate_problem":
                        used_models.append("o4-mini")
                    elif function_name == "evaluate_writing":
                        used_models.append("o4-mini")

                    # Function ì‹¤í–‰
                    function_response = self._execute_function(function_name, arguments)

                    # ë“£ê¸° ë¬¸ì œì˜ ê²½ìš°: [AUDIO]ì™€ [SPEAKERS] íƒœê·¸ë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•´ ì§ì ‘ ë°˜í™˜
                    if function_name == "generate_problem" and arguments.get("area", "").lower() in ['ë“£ê¸°', 'listening', 'ls']:
                        # Check if response contains [AUDIO] or [SPEAKERS]
                        if '[AUDIO]' in function_response or '[SPEAKERS]' in function_response:
                            print("ğŸ§ ë“£ê¸° ë¬¸ì œ: [AUDIO]/[SPEAKERS] íƒœê·¸ ë³´ì¡´ì„ ìœ„í•´ ì§ì ‘ ë°˜í™˜")
                            return {
                                "message": function_response,
                                "model_info": {
                                    "primary": "o4-mini",
                                    "all_used": list(set(used_models))
                                }
                            }

                    # Function ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": function_response
                    })

                # Function ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
                # Use the same model as primary for consistency
                if primary_model == "o4-mini":
                    final_response = self.client.chat.completions.create(
                        model="o4-mini",
                        messages=messages,
                        max_completion_tokens=10000
                    )
                else:
                    final_response = self.client.chat.completions.create(
                        model="gpt-4.1-mini",
                        messages=messages,
                        temperature=0.7
                    )

                response_content = final_response.choices[0].message.content

                # Step 4: Quality check for o4-mini responses
                if primary_model == "o4-mini" and not self._check_response_quality(response_content):
                    print(f"âš ï¸  o4-mini response quality low, falling back to o3...")
                    used_models.append("o3")

                    # Retry with o3 (advanced reasoning)
                    try:
                        final_response_o3 = self.client.chat.completions.create(
                            model="o3",
                            messages=messages,
                            max_completion_tokens=10000
                        )
                        response_content = final_response_o3.choices[0].message.content
                        primary_model = "o3"  # Update primary model
                        print(f"âœ… o3 response generated successfully")
                    except Exception as e:
                        print(f"âš ï¸  o3 fallback failed: {e}, using o4-mini response anyway")

                return self._parse_quick_reply(response_content, list(set(used_models)))
            else:
                # Function í˜¸ì¶œ ì—†ì´ ì§ì ‘ ì‘ë‹µ
                response_content = assistant_message.content

                # Step 4: Quality check for o4-mini responses (no function calls)
                if primary_model == "o4-mini" and not self._check_response_quality(response_content):
                    print(f"âš ï¸  o4-mini response quality low, falling back to o3...")
                    used_models.append("o3")

                    # Retry with o3 (advanced reasoning)
                    try:
                        final_response_o3 = self.client.chat.completions.create(
                            model="o3",
                            messages=messages,
                            max_completion_tokens=10000
                        )
                        response_content = final_response_o3.choices[0].message.content
                        primary_model = "o3"  # Update primary model
                        print(f"âœ… o3 response generated successfully")
                    except Exception as e:
                        print(f"âš ï¸  o3 fallback failed: {e}, using o4-mini response anyway")

                return self._parse_quick_reply(response_content, list(set(used_models)))

        except Exception as e:
            return {
                "message": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "model_info": {"primary": "error", "all_used": []}
            }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_student_agent_service = None


def get_student_agent_service() -> StudentAgentService:
    """Student Agent ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _student_agent_service
    if _student_agent_service is None:
        _student_agent_service = StudentAgentService()
    return _student_agent_service
